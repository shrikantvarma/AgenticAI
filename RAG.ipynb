{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPlacYKEL9SqkUGgp1Q3uK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTCv9eID_Fgc"
      },
      "outputs": [],
      "source": [
        "!pip install openai chromadb PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40c1c87c"
      },
      "source": [
        "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system. It extracts text from a PDF, chunks the text, creates embeddings using OpenAI, stores them in ChromaDB, and then uses the stored embeddings to retrieve relevant context for answering questions using an OpenAI GPT model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import chromadb\n",
        "from PyPDF2 import PdfReader\n",
        "\n"
      ],
      "metadata": {
        "id": "sOJVLuDT_KTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3ah_3gE_Ogz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract text\n",
        "def extract_text_from_pdf(path):\n",
        "    reader = PdfReader(path)\n",
        "    return \" \".join([page.extract_text() for page in reader.pages])\n",
        "\n",
        "text = extract_text_from_pdf(\"PRD.pdf\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElR5H7wr_WSw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Chunk text\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "chunks = chunk_text(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "d8fkpq-R_boQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create embeddings\n",
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.get_or_create_collection(name=\"docs\")\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    embedding = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=chunk\n",
        "    ).data[0].embedding\n",
        "    collection.add(documents=[chunk], embeddings=[embedding], ids=[str(i)])"
      ],
      "metadata": {
        "id": "lup6uhEdBJNH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Query + Retrieval\n",
        "def query_doc(question):\n",
        "    q_embed = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\", input=question\n",
        "    ).data[0].embedding\n",
        "\n",
        "    results = collection.query(query_embeddings=[q_embed], n_results=3)\n",
        "    context = \" \".join(results[\"documents\"][0])\n",
        "\n",
        "    # Step 5: Ask GPT\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Use the provided context to answer factually.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(query_doc(\"What is the key takeaway from this paper?\"))"
      ],
      "metadata": {
        "id": "VTewThlPBLGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cc2d1a-286c-47fc-da2f-9e2755164b56"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The key takeaway from the paper is the implementation of a robust evaluation and cost optimization strategy for enhancing the performance of a large language model (LLM) search system. The strategy includes carefully testing changes on a small portion of traffic, utilizing multiple models (like GPT-4 and GPT-3.5) for different query complexities, and employing techniques such as response caching, prompt optimization, and offline batch evaluation to significantly reduce operating costs while maintaining high quality and reliability in responses. This ensures that the system continuously improves based on defined metrics and user outcomes, ultimately providing value to stakeholders while managing financial resources effectively.\n"
          ]
        }
      ]
    }
  ]
}