{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrikantvarma/AgenticAI/blob/main/hybrid_rag_real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c22768"
      },
      "source": [
        "# Hybrid RAG: Traditional vs Agentic RAG\n",
        "\n",
        "**A Practical Comparison with ChromaDB and OpenAI**\n",
        "\n",
        "This notebook explores **Hybrid Retrieval-Augmented Generation (RAG)**, demonstrating how it can intelligently combine the strengths of **Traditional RAG** and **Agentic RAG** approaches. By routing queries based on their complexity, Hybrid RAG aims to optimize the balance between **latency** (speed) and **quality** (accuracy and depth of analysis) for different types of user questions.\n",
        "\n",
        "\n",
        "\n",
        "We use an **Admin trigger troubleshooting use case** to illustrate these concepts, leveraging:\n",
        "-   **ChromaDB**: A vector database for efficient storage and retrieval of troubleshooting knowledge.\n",
        "-   **OpenAI GPT-4o-mini**: A powerful language model for generating responses and executing diagnostic steps.\n",
        "\n",
        "The scenario is as follows:\n",
        "1. There is a trigger in the system that failed.\n",
        "2. There is a document that explains how to troubleshoot triggers.\n",
        "3. The trigger setting and logs are available to the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## Understanding the RAG Approaches\n",
        "\n",
        "-   **Traditional RAG**:\n",
        "    -   **Process**: Simple, single-step approach. Retrieves relevant documents from a vector database and uses an LLM to generate an answer based *only* on the retrieved context.\n",
        "    -   **Characteristics**: Generally **fast** and **low cost** due to minimal LLM interaction. Provides **generic answers** without analyzing specific system states.\n",
        "\n",
        "-   **Agentic RAG**:\n",
        "    -   **Process**: A multi-step reasoning process that involves multiple LLM calls. It retrieves relevant information from the troubleshooting guides *and* analyzes provided state data (like trigger settings and logs) to form a diagnosis. The LLM acts as an \"agent\" to plan and execute checks.\n",
        "    -   **Characteristics**: **Thorough** and provides **specific solution**. However, it is **slower** and **more costly** due to the increased number of steps and LLM interactions.\n",
        "\n",
        "-   **Hybrid RAG**:\n",
        "    -   **Process**: Introduces an intelligent router that assesses the **complexity** of the incoming user query. Simple queries are directed to the faster Traditional RAG path, while complex queries requiring deeper analysis are sent to the more thorough Agentic RAG path.\n",
        "    -   **Characteristics**: Aims to achieve the **best balance** between speed and quality by using the most appropriate method for each query type.\n",
        "\n",
        "---\n",
        "\n",
        "## Demonstration and Comparison\n",
        "\n",
        "The notebook demonstrates these approaches by:\n",
        "1.  Setting up a knowledge base in ChromaDB and simulating trigger state data.\n",
        "2.  Implementing and running both Traditional and Agentic RAG methods on a troubleshooting query.\n",
        "3.  Comparing their performance metrics (Latency, LLM Calls) and output quality.\n",
        "4.  Implementing a simple Hybrid RAG router and testing how it routes different types of queries.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "-   No single RAG method is ideal for all situations; there's an inherent **latency vs quality tradeoff**.\n",
        "-   Traditional RAG is effective for quick information retrieval (simple queries).\n",
        "-   Agentic RAG is powerful for complex problem-solving requiring state analysis.\n",
        "-   **Hybrid RAG** provides a practical solution to this tradeoff by dynamically choosing the optimal approach per query.\n",
        "-   Effective **query complexity assessment** is fundamental to a successful Hybrid RAG implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e53695e"
      },
      "source": [
        "## Comparison Metrics and Summary\n",
        "\n",
        "The demonstration highlights the differences between the RAG approaches based on the following metrics observed during the runs:\n",
        "\n",
        "-   **Quality**: The relevance, specificity, and depth of the generated answer.\n",
        "-   **Latency**: The time taken to generate a response.\n",
        "-   **LLM Calls**: The number of times the language model is invoked.\n",
        "-   **Cost**: An estimation of the cost based on the number of LLM calls and assumed token usage.\n",
        "-   **Complexity Handled**: The type of queries each method is best suited for.\n",
        "-   **State Analysis**: Whether the method incorporates analysis of the provided system state.\n",
        "-   **Steps**: The number and nature of the steps involved in generating a response.\n",
        "\n",
        "Here is a summary table comparing the approaches with metrics from the runs:\n",
        "\n",
        "| Characteristic     | Traditional RAG                 | Agentic RAG                     | Hybrid RAG                      |\n",
        "| :----------------- | :------------------------------ | :------------------------------ | :------------------------------ |\n",
        "| **Quality**        | Generic answers                 | Specific diagnosis              | Varies based on routing         |\n",
        "| **Latency**        | 3766 ms                        | 14316 ms                       | 8721 ms (average)              |\n",
        "| **LLM Calls**      | 1                               | 6                               | ~3.5 (average)                  |\n",
        "| **Estimated Cost** | 0.02 cents                         | 0.12 cents              | 0.07 cents                          |\n",
        "| **Complexity Handled** | Simple queries                  | Complex queries                 | Routes based on query complexity |\n",
        "| **State Analysis** | No                              | Yes                             | Yes (when routed to Agentic)    |\n",
        "| **Steps**          | Single retrieval and generation | Multi-step reasoning and checks | Routes to appropriate method    |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-HjpylNCW5X",
        "outputId": "daf046ee-14c7-4875-b1de-cd497075ec73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "!pip install chromadb openai python-dotenv -q\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGDcrQ4CW5X",
        "outputId": "3b6dd476-3e67-468d-b467-15e2b774aa8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Imports loaded\n",
            "✓ OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Setup and Imports\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "print(\"✓ Imports loaded\")\n",
        "print(\"✓ OpenAI client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHUS8AycCW5Y",
        "outputId": "099eac8f-4e62-4df6-eb95-134875a0700c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Knowledge base created\n",
            "  - 4 troubleshooting documents\n",
            "  - Trigger state with 3 logs\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Create Knowledge Base\n",
        "# ============================================================================\n",
        "\n",
        "# Troubleshooting guide as a single string\n",
        "troubleshooting_guide_text = \"\"\"Trigger Disabled Issue: If trigger is not firing, first check if the trigger is enabled.\n",
        "Go to Settings > Automation > Triggers and look for the toggle switch. It should be ON/green.\n",
        "Disabled triggers never fire regardless of conditions.\n",
        "\n",
        "Condition Matching: Triggers only fire when conditions match ticket data.\n",
        "With ALL logic, every condition must be true. With ANY logic, at least one condition must be true.\n",
        "Compare your trigger conditions against the actual ticket field values carefully.\n",
        "\n",
        "Logic Types Explained: ALL logic means every condition must match (AND).\n",
        "For example, status=new AND priority=high means both must be true.\n",
        "ANY logic means at least one condition must match (OR).\n",
        "Common mistake: using ALL when you meant ANY.\n",
        "\n",
        "Execution Logs: Check trigger logs to see which tickets were evaluated,\n",
        "whether the trigger fired, and the specific reason it didn't fire.\n",
        "Logs are found in Settings > Automation > Logs and show execution history with error details.\n",
        "\"\"\"\n",
        "\n",
        "# Split the guide into individual documents based on paragraphs\n",
        "troubleshooting_docs_content = troubleshooting_guide_text.strip().split('\\n\\n')\n",
        "\n",
        "# Create a list of document dictionaries with IDs and metadata\n",
        "troubleshooting_docs = [\n",
        "    {\n",
        "        \"id\": f\"doc_{i+1}\",\n",
        "        \"content\": content,\n",
        "        \"metadata\": {\"category\": \"troubleshooting\"} # Using a generic category for now\n",
        "    }\n",
        "    for i, content in enumerate(troubleshooting_docs_content)\n",
        "]\n",
        "\n",
        "\n",
        "# Trigger state (actual settings and logs)\n",
        "trigger_state = {\n",
        "    \"trigger_settings\": {\n",
        "        \"id\": \"trigger_001\",\n",
        "        \"name\": \"Auto-assign high priority tickets\",\n",
        "        \"enabled\": True,\n",
        "        \"conditions\": [\n",
        "            {\"field\": \"status\", \"operator\": \"equals\", \"value\": \"new\"},\n",
        "            {\"field\": \"priority\", \"operator\": \"equals\", \"value\": \"high\"}\n",
        "        ],\n",
        "        \"logic\": \"ALL\",\n",
        "        \"actions\": [\"assign_to_team_a\"]\n",
        "    },\n",
        "    \"execution_logs\": [\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_123\",\n",
        "            \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: priority is 'medium', expected 'high'\"\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_124\",\n",
        "            \"timestamp\": \"2025-01-15T11:15:00Z\",\n",
        "            \"fired\": True,\n",
        "            \"actions_executed\": [\"Assigned to Team A\"]\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_125\",\n",
        "            \"timestamp\": \"2025-01-15T14:22:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: status is 'open', expected 'new'\"\n",
        "        }\n",
        "    ],\n",
        "    \"recent_tickets\": [\n",
        "        {\"id\": \"TKT_123\", \"status\": \"new\", \"priority\": \"medium\"},\n",
        "        {\"id\": \"TKT_124\", \"status\": \"new\", \"priority\": \"high\"},\n",
        "        {\"id\": \"TKT_125\", \"status\": \"open\", \"priority\": \"high\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"✓ Knowledge base created\")\n",
        "print(f\"  - {len(troubleshooting_docs)} troubleshooting documents\")\n",
        "print(f\"  - Trigger state with {len(trigger_state['execution_logs'])} logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crlwWDY3CW5Y",
        "outputId": "106e1038-7130-4e80-828e-9f4851708b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ChromaDB initialized\n",
            "  - Collection: trigger_troubleshooting\n",
            "  - Documents: 4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Setup ChromaDB Vector Database\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Reset to start fresh\n",
        "chroma_client.reset()\n",
        "\n",
        "# Create collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"trigger_troubleshooting\",\n",
        "    metadata={\"description\": \"Admin trigger troubleshooting knowledge base\"}\n",
        ")\n",
        "\n",
        "# Add documents to collection\n",
        "collection.add(\n",
        "    documents=[doc[\"content\"] for doc in troubleshooting_docs],\n",
        "    ids=[doc[\"id\"] for doc in troubleshooting_docs],\n",
        "    metadatas=[doc[\"metadata\"] for doc in troubleshooting_docs]\n",
        ")\n",
        "\n",
        "print(\"✓ ChromaDB initialized\")\n",
        "print(f\"  - Collection: {collection.name}\")\n",
        "print(f\"  - Documents: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC15P7FqCW5Y",
        "outputId": "4081bb1c-e5eb-43da-953b-80c7c9460d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Helper functions ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Helper Functions for LLM and Retrieval\n",
        "# ============================================================================\n",
        "\n",
        "def call_llm(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 500) -> str:\n",
        "    \"\"\"Call OpenAI API\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def retrieve_docs(query: str, n_results: int = 2) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant documents from ChromaDB\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        docs.append({\n",
        "            'id': results['ids'][0][i],\n",
        "            'content': results['documents'][0][i],\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'distance': results['distances'][0][i] if 'distances' in results else None\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "print(\"✓ Helper functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-rI7RoCW5Y",
        "outputId": "138e8db0-e2af-4e66-d8b7-cb47e627d6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Traditional RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Traditional RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class TraditionalRAG:\n",
        "    \"\"\"\n",
        "    Traditional RAG: Single retrieval + single generation\n",
        "    - Retrieve relevant docs from vector DB\n",
        "    - Generate answer in one LLM call\n",
        "    - Fast but generic (no state analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        docs = retrieve_docs(query, n_results=2)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "        prompt = f\"\"\"You are a helpful admin assistant. Answer the user's question based on this troubleshooting guide.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Troubleshooting Guide:\n",
        "{context}\n",
        "\n",
        "Provide a helpful answer:\"\"\"\n",
        "\n",
        "        answer = call_llm(prompt)\n",
        "        steps.append(\"Generated answer with LLM\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Traditional RAG\",\n",
        "            \"retrieval_calls\": 1,\n",
        "            \"llm_calls\": 1,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "trad_rag = TraditionalRAG()\n",
        "print(\"✓ Traditional RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpGeXub4CW5Y",
        "outputId": "dfc1c95f-236f-4d31-9b0a-cf3e3d102912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Agentic RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Agentic RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Agentic RAG: Multi-step reasoning with state analysis\n",
        "    - Retrieve troubleshooting guide\n",
        "    - Create diagnostic plan (LLM call #1)\n",
        "    - Execute state checks (LLM calls #2-5)\n",
        "    - Synthesize diagnosis (LLM call #6)\n",
        "    - Slower but provides specific diagnosis\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str, state: Dict = None) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        llm_calls = 0\n",
        "        retrieval_calls = 0\n",
        "\n",
        "        if state is None:\n",
        "            state = trigger_state\n",
        "\n",
        "        # Step 1: Retrieve troubleshooting docs\n",
        "        docs = retrieve_docs(query, n_results=3)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "        retrieval_calls += 1\n",
        "\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "\n",
        "        # Step 2: Create diagnostic plan (LLM call #1)\n",
        "        plan_prompt = f\"\"\"Based on the user's question and troubleshooting guide, what specific checks should we run?\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Guide:\n",
        "{context}\n",
        "\n",
        "Return ONLY a JSON array of 4 specific checks to run against the trigger state. Format:\n",
        "[\"check_description_1\", \"check_description_2\", \"check_description_3\", \"check_description_4\"]\"\"\"\n",
        "\n",
        "        plan_response = call_llm(plan_prompt, max_tokens=200)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Created diagnostic plan\")\n",
        "\n",
        "        try:\n",
        "            diagnostic_plan = json.loads(plan_response)\n",
        "        except:\n",
        "            diagnostic_plan = [\"Check enabled\", \"Check logs\", \"Check conditions\", \"Check logic\"]\n",
        "\n",
        "        # Step 3: Execute each diagnostic check\n",
        "        findings = []\n",
        "\n",
        "        # Check 1: Is enabled?\n",
        "        check_prompt = f\"\"\"Check if trigger is enabled.\n",
        "\n",
        "Trigger settings: {json.dumps(state['trigger_settings'], indent=2)}\n",
        "\n",
        "Answer in one sentence: Is the trigger enabled?\"\"\"\n",
        "        finding = call_llm(check_prompt, max_tokens=50)\n",
        "        findings.append(f\"Enabled status: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #1: Verified enabled status\")\n",
        "\n",
        "        # Check 2: Analyze logs\n",
        "        logs_prompt = f\"\"\"Analyze these execution logs.\n",
        "\n",
        "Logs: {json.dumps(state['execution_logs'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: What do the logs show about trigger firing?\"\"\"\n",
        "        finding = call_llm(logs_prompt, max_tokens=100)\n",
        "        findings.append(f\"Log analysis: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #2: Analyzed execution logs\")\n",
        "\n",
        "        # Check 3: Compare conditions\n",
        "        conditions_prompt = f\"\"\"Compare trigger conditions against actual tickets.\n",
        "\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "Recent tickets: {json.dumps(state['recent_tickets'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: Which tickets match the conditions?\"\"\"\n",
        "        finding = call_llm(conditions_prompt, max_tokens=150)\n",
        "        findings.append(f\"Condition matching: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #3: Compared conditions vs tickets\")\n",
        "\n",
        "        # Check 4: Verify logic type\n",
        "        logic_prompt = f\"\"\"Explain the logic type.\n",
        "\n",
        "Logic type: {state['trigger_settings']['logic']}\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "\n",
        "Answer in 1-2 sentences: What does this logic type mean?\"\"\"\n",
        "        finding = call_llm(logic_prompt, max_tokens=100)\n",
        "        findings.append(f\"Logic type: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #4: Verified logic type\")\n",
        "\n",
        "        # Step 4: Synthesize final diagnosis (LLM call #6)\n",
        "        synthesis_prompt = f\"\"\"Based on all the findings, provide a specific diagnosis.\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Findings:\n",
        "{chr(10).join(f'{i+1}. {f}' for i, f in enumerate(findings))}\n",
        "\n",
        "Provide a specific, actionable answer that:\n",
        "1. Explains if the trigger is working correctly or not\n",
        "2. Gives evidence from the actual state\n",
        "3. Explains why certain tickets didn't fire\n",
        "4. Suggests concrete next steps if needed\"\"\"\n",
        "\n",
        "        answer = call_llm(synthesis_prompt, max_tokens=600)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Synthesized final diagnosis\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Agentic RAG\",\n",
        "            \"retrieval_calls\": retrieval_calls,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"findings\": findings,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "agentic_rag = AgenticRAG()\n",
        "print(\"✓ Agentic RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb1nEW_ZCW5Y",
        "outputId": "6f35de32-b314-4368-df3e-ac04aa02ba40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Hybrid RAG router ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Hybrid RAG Router\n",
        "# ============================================================================\n",
        "\n",
        "class HybridRAG:\n",
        "    \"\"\"\n",
        "    Hybrid RAG: Intelligent routing\n",
        "    - Simple queries → Traditional RAG\n",
        "    - Complex queries → Agentic RAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, complexity_threshold: float = 0.5):\n",
        "        self.traditional = TraditionalRAG()\n",
        "        self.agentic = AgenticRAG()\n",
        "        self.threshold = complexity_threshold\n",
        "\n",
        "    def assess_complexity(self, query: str) -> float:\n",
        "        \"\"\"Assess query complexity (0.0 = simple, 1.0 = complex)\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Simple queries\n",
        "        if any(word in query_lower for word in [\"where\", \"what is\", \"how to find\"]):\n",
        "            return 0.3\n",
        "\n",
        "        # Complex queries\n",
        "        if any(word in query_lower for word in [\"why\", \"not working\", \"not firing\", \"issue\"]):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.5\n",
        "\n",
        "    def answer(self, query: str, verbose: bool = True) -> Dict:\n",
        "        complexity = self.assess_complexity(query)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Complexity: {complexity:.2f} (threshold: {self.threshold})\")\n",
        "\n",
        "        if complexity < self.threshold:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Traditional RAG (simple query)\\n\")\n",
        "            result = self.traditional.answer(query)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Agentic RAG (complex query)\\n\")\n",
        "            result = self.agentic.answer(query)\n",
        "\n",
        "        result['complexity'] = complexity\n",
        "        return result\n",
        "\n",
        "hybrid = HybridRAG(complexity_threshold=0.5)\n",
        "print(\"✓ Hybrid RAG router ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmB8_DwCW5Y",
        "outputId": "1e4df6d8-4a99-4afd-c708-771a54366e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPARISON: Traditional RAG vs Agentic RAG\n",
            "======================================================================\n",
            "Query: Why isn't my trigger firing?\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "METHOD 1: Traditional RAG\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 2 documents from ChromaDB\n",
            "  2. Generated answer with LLM\n",
            "\n",
            "Answer:\n",
            "There could be a couple of reasons why your trigger isn't firing. First, check if the trigger is enabled. You can do this by navigating to Settings > Automation > Triggers and looking for the toggle switch next to your trigger. It should be ON (green); if it's not, simply enable it.\n",
            "\n",
            "If the trigger is enabled, the next step is to ensure that the conditions you've set match the ticket data. Remember that with ALL logic, every condition must be true for the trigger to fire, while with ANY logic, at least one condition must be true. Carefully compare your trigger conditions against the actual values in the ticket fields to ensure they align correctly.\n",
            "\n",
            "If you've checked both of these aspects and the trigger still isn't firing, there may be other factors at play, but these are the most common issues to investigate first.\n",
            "\n",
            "📊 Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 1\n",
            "   Latency: 3765.82ms\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "METHOD 2: Agentic RAG\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 3 documents from ChromaDB\n",
            "  2. Created diagnostic plan\n",
            "  3. Check #1: Verified enabled status\n",
            "  4. Check #2: Analyzed execution logs\n",
            "  5. Check #3: Compared conditions vs tickets\n",
            "  6. Check #4: Verified logic type\n",
            "  7. Synthesized final diagnosis\n",
            "\n",
            "Answer:\n",
            "### Diagnosis of Trigger Firing Issue\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly based on the defined conditions. It is enabled and has successfully fired for ticket TKT_124, which meets all specified criteria.\n",
            "\n",
            "2. **Evidence from Actual State**: \n",
            "   - The logs show that the trigger executed successfully for TKT_124, assigning it to Team A. \n",
            "   - The other two tickets, TKT_123 and TKT_125, did not trigger because they did not meet the required conditions. TKT_123 had a different priority, and TKT_125 had a different status.\n",
            "\n",
            "3. **Explanation for Non-Firing Tickets**:\n",
            "   - **TKT_123**: This ticket did not fire because its priority did not match the required \"high\" priority condition.\n",
            "   - **TKT_125**: This ticket did not fire because its status did not match the required \"new\" status condition.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Evaluate the conditions for TKT_123 and TKT_125 to determine if they can be adjusted to meet the trigger criteria. If these tickets should also trigger actions, consider updating their priority or status accordingly.\n",
            "   - **Adjust Trigger Logic**: If it is common for tickets to have varying priorities and statuses, consider modifying the trigger conditions or logic type. For example, if you want to fire the trigger for tickets with either \"new\" status or \"high\" priority, you might switch from \"ALL\" to \"ANY\" logic.\n",
            "   - **Monitor Future Tickets**: Continue to monitor the logs for future tickets to ensure that the trigger fires as expected and adjust conditions as necessary based on ticket trends. \n",
            "\n",
            "By following these steps, you can ensure that the trigger works effectively for all relevant tickets.\n",
            "\n",
            "📊 Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 6\n",
            "   Latency: 14316.45ms\n",
            "\n",
            "======================================================================\n",
            "TRADEOFF ANALYSIS\n",
            "======================================================================\n",
            "⚡ Speed: Traditional is 3.8x faster\n",
            "🤖 LLM calls: Agentic uses 6x more\n",
            "🎯 Quality: Agentic provides specific diagnosis with state analysis\n",
            "💰 Cost: Agentic uses 6x more tokens\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Run Comparison Demo\n",
        "# ============================================================================\n",
        "\n",
        "def compare_methods(query: str):\n",
        "    \"\"\"Compare Traditional vs Agentic RAG\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON: Traditional RAG vs Agentic RAG\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Query: {query}\\n\")\n",
        "\n",
        "    # Traditional\n",
        "    print(\"─\"*70)\n",
        "    print(\"METHOD 1: Traditional RAG\")\n",
        "    print(\"─\"*70)\n",
        "    trad_result = trad_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(trad_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{trad_result['answer']}\")\n",
        "    print(f\"\\n📊 Performance:\")\n",
        "    print(f\"   Retrieval calls: {trad_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {trad_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {trad_result['latency_ms']}ms\")\n",
        "\n",
        "    # Agentic\n",
        "    print(\"\\n\" + \"─\"*70)\n",
        "    print(\"METHOD 2: Agentic RAG\")\n",
        "    print(\"─\"*70)\n",
        "    agentic_result = agentic_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(agentic_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{agentic_result['answer']}\")\n",
        "    print(f\"\\n📊 Performance:\")\n",
        "    print(f\"   Retrieval calls: {agentic_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {agentic_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {agentic_result['latency_ms']}ms\")\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRADEOFF ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    speedup = agentic_result['latency_ms'] / trad_result['latency_ms']\n",
        "    print(f\"⚡ Speed: Traditional is {speedup:.1f}x faster\")\n",
        "    print(f\"🤖 LLM calls: Agentic uses {agentic_result['llm_calls']}x more\")\n",
        "    print(f\"🎯 Quality: Agentic provides specific diagnosis with state analysis\")\n",
        "    print(f\"💰 Cost: Agentic uses {agentic_result['llm_calls']}x more tokens\")\n",
        "\n",
        "# Run comparison\n",
        "compare_methods(\"Why isn't my trigger firing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcNxQ-s1CW5Z",
        "outputId": "f79056c7-49e4-420c-d125-6f5844946c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "HYBRID RAG ROUTER TEST\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Query: Where can I find triggers?\n",
            "======================================================================\n",
            "Complexity: 0.30 (threshold: 0.5)\n",
            "→ Routing to: Traditional RAG (simple query)\n",
            "\n",
            "\n",
            "Method chosen: Traditional RAG\n",
            "Final Answer:\n",
            "You can find triggers by navigating to **Settings > Automation > Triggers**. Here, you can view all your triggers and check if they are enabled. Make sure the toggle switch next to each trigger is ON (green) to ensure they are active. If you're experiencing issues with triggers not firing, you can also check the execution logs for more details by going to **Settings > Automation > Logs**. This will show you which tickets were evaluated and any error details if the trigger did not fire.\n",
            "Performance: 1 LLM calls, 2388.4ms\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: My trigger isn't firing, what's wrong?\n",
            "======================================================================\n",
            "Complexity: 0.50 (threshold: 0.5)\n",
            "→ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Issue\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly based on the defined conditions. It is enabled and is firing as expected for tickets that meet the specified criteria.\n",
            "\n",
            "2. **Evidence from Actual State**: The analysis of the logs shows that out of three processed tickets, only TKT_124 met the conditions required for the trigger to fire. TKT_124 had a status of \"new\" and a priority of \"high,\" which aligns perfectly with the trigger's requirements. The other two tickets, TKT_123 and TKT_125, did not meet the criteria, which is why they did not result in a trigger firing.\n",
            "\n",
            "3. **Explanation of Non-Firing Tickets**:\n",
            "   - **TKT_123**: This ticket did not fire because it did not have the required priority of \"high.\" It likely has a lower priority, which is a condition mismatch.\n",
            "   - **TKT_125**: This ticket did not fire due to a status mismatch. Its status does not match the required \"new\" status, which is necessary for the trigger to activate.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Examine the conditions set for the trigger to ensure they align with your intended use case. If you want other tickets to fire, you may need to adjust the conditions to include different statuses or priorities.\n",
            "   - **Modify Trigger Logic**: If you want to capture more tickets, consider changing the logic type from \"ALL\" to \"ANY\" if applicable, which would allow tickets to fire if they meet at least one of the conditions.\n",
            "   - **Monitor Future Tickets**: Continue monitoring new tickets to see if any meet the criteria. If you frequently encounter tickets that do not match the current conditions, it may be worth reassessing the trigger's logic and conditions to ensure it aligns with your operational needs.\n",
            "\n",
            "By following these steps, you can enhance the effectiveness of the trigger and ensure it captures the relevant tickets as intended.\n",
            "Performance: 6 LLM calls, 15054.17ms\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: Why did trigger fire for some tickets but not others?\n",
            "======================================================================\n",
            "Complexity: 0.80 (threshold: 0.5)\n",
            "→ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Functionality\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly based on the defined conditions. It successfully fired for ticket TKT_124, which met all specified criteria.\n",
            "\n",
            "2. **Evidence from Actual State**: \n",
            "   - The logs confirm that the trigger is enabled and processed three tickets.\n",
            "   - TKT_124 had a status of \"new\" and a priority of \"high,\" which are the exact conditions required for the trigger to fire. \n",
            "   - In contrast, TKT_123 and TKT_125 did not meet the necessary conditions: TKT_123 had a different priority, and TKT_125 had a status that did not match \"new.\"\n",
            "\n",
            "3. **Explanation for Non-Firing Tickets**:\n",
            "   - **TKT_123**: Did not fire because it did not meet the priority condition. The priority was likely set to \"medium\" or \"low,\" which disqualified it from triggering the action.\n",
            "   - **TKT_125**: Did not fire due to its status not being \"new.\" This indicates that it may have been in a different state, such as \"in progress\" or \"resolved,\" which does not satisfy the trigger's requirements.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Examine the conditions for TKT_123 and TKT_125 to understand their statuses and priorities. If these tickets are expected to trigger actions, consider adjusting their attributes to meet the criteria.\n",
            "   - **Adjust Trigger Logic (if necessary)**: If there are other scenarios where tickets should trigger actions, consider modifying the trigger logic to accommodate additional statuses or priorities.\n",
            "   - **Monitor Future Tickets**: Continue to monitor the logs for future ticket processing to ensure that the trigger behaves as expected and to identify any other tickets that may not meet the criteria. \n",
            "\n",
            "By following these steps, you can ensure that the trigger operates effectively and that all relevant tickets are processed as intended.\n",
            "Performance: 6 LLM calls, 15043.64ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Test Hybrid Router\n",
        "# ============================================================================\n",
        "\n",
        "test_queries = [\n",
        "    \"Where can I find triggers?\",\n",
        "    \"My trigger isn't firing, what's wrong?\",\n",
        "    \"Why did trigger fire for some tickets but not others?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYBRID RAG ROUTER TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for query in test_queries:\n",
        "    result = hybrid.answer(query, verbose=True)\n",
        "    print(f\"\\nMethod chosen: {result['method']}\")\n",
        "    print(f\"Final Answer:\\n{result['answer']}\") # Added to display the final answer\n",
        "    print(f\"Performance: {result['llm_calls']} LLM calls, {result['latency_ms']}ms\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYfoS4zWW0Sn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
