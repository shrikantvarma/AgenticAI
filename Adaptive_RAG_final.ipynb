{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrikantvarma/AgenticAI/blob/main/Adaptive_RAG_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0854ebc6"
      },
      "source": [
        "# Adaptive/Hybrid RAG: Traditional +  Agentic RAG\n",
        "\n",
        "**A Practical Comparison with ChromaDB and OpenAI**\n",
        "\n",
        "This notebook explores **Adaptive/Hybrid Retrieval-Augmented Generation (RAG)**, demonstrating how it can intelligently combine the strengths of **Traditional RAG** and **Agentic RAG** approaches. By routing queries based on their complexity, Adaptive RAG aims to optimize the balance between **latency** (speed) and **quality** (accuracy and depth of analysis) for different types of user questions.\n",
        "\n",
        "We use an **Admin trigger troubleshooting use case** to illustrate these concepts, leveraging:\n",
        "- **ChromaDB**: A vector database for efficient storage and retrieval of troubleshooting knowledge.\n",
        "- **OpenAI GPT-4o-mini**: A powerful language model for generating responses and executing diagnostic steps.\n",
        "\n",
        "The scenario is as follows:\n",
        "1. A trigger in the system has failed. The user wants to troubleshoot it.\n",
        "2. There is a document that explains how to troubleshoot triggers.\n",
        "3. The trigger setting and logs are available to the LLM.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Understanding the RAG Approaches\n",
        "\n",
        "- **Traditional RAG**:\n",
        "  - **Process**: Simple, single-step approach. Retrieves relevant documents from a vector database and uses an LLM to generate an answer based *only* on the retrieved context.\n",
        "  - **Characteristics**: Generally **fast** and **low cost** due to minimal LLM interaction. Provides **generic answers** that are directionally correct without analyzing specific system states.\n",
        "- **Agentic RAG**:\n",
        "  - **Process**: A multi-step reasoning process that involves multiple LLM calls. It retrieves relevant information from the troubleshooting guides, creates a troubleshooting plan dynamically *and* analyzes the system information to provide an appropriate solution. The LLM acts as an \"agent\" to plan and execute checks.\n",
        "  - **Characteristics**: **Thorough** and provides **specific solution**. However, it is **slower** and **more costly** due to the increased number of steps and LLM interactions.\n",
        "- **Adaptive RAG**:\n",
        "  - **Process**: Introduces an intelligent router that assesses the **complexity** of the incoming user query. Simple queries are directed to the faster Traditional RAG path, while complex queries requiring deeper analysis are sent to the more thorough Agentic RAG path.\n",
        "  - **Characteristics**: Aims to achieve the **best balance** between speed and quality by using the most appropriate method for each query type.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Demonstration and Comparison\n",
        "\n",
        "The notebook demonstrates these approaches by:\n",
        "1. Setting up a knowledge base in ChromaDB and simulating trigger state data.\n",
        "2. Implementing and running both Traditional and Agentic RAG methods on a troubleshooting query.\n",
        "3. Comparing their performance metrics (Latency, LLM Calls) and output quality, **including evaluation of output quality using an LLM as a judge.**\n",
        "4. Implementing a simple Hybrid RAG router and testing how it routes different types of queries.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- No single RAG method is ideal for all situations; there's an inherent **latency vs quality tradeoff**.\n",
        "- Traditional RAG is effective for quick information retrieval (simple queries).\n",
        "- Agentic RAG is powerful for complex problem-solving requiring state analysis.\n",
        "- **Hybrid RAG** provides a practical solution to this tradeoff by dynamically choosing the optimal approach per query.\n",
        "- Effective **query complexity assessment** is fundamental to a successful Hybrid RAG implementation.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Summary of Key Findings\n",
        "\n",
        "Based on the demonstration and evaluation, the following key findings highlight the tradeoffs and benefits of each RAG approach:\n",
        "\n",
        "1.  **Traditional RAG**:\n",
        "    *   **Performance**: Significantly faster and uses fewer LLM calls (and thus lower estimated cost).\n",
        "    *   **Quality**: Provides general answers based on the retrieved troubleshooting guide. It is effective for simple, directional queries (e.g., \"Where can I find triggers?\"). However, for complex troubleshooting queries, it lacks the ability to analyze the specific system state, resulting in a lower LLM evaluation score when compared to Agentic RAG using criteria that value specific diagnosis.\n",
        "    *   **Best Use Case**: Simple queries requiring information retrieval from the knowledge base.\n",
        "\n",
        "2.  **Agentic RAG**:\n",
        "    *   **Performance**: Slower and uses multiple LLM calls (higher estimated cost) due to the multi-step reasoning and state analysis process.\n",
        "    *   **Quality**: Provides a more specific, in-depth diagnosis by analyzing the provided trigger state (settings and logs) in addition to the troubleshooting guide. This leads to a higher LLM evaluation score for complex troubleshooting queries, as it offers concrete explanations for *why* a trigger isn't firing for specific tickets and suggests actionable next steps based on the state.\n",
        "    *   **Best Use Case**: Complex queries requiring detailed analysis of system state and specific problem diagnosis.\n",
        "\n",
        "3.  **Adaptive (Hybrid) RAG**:\n",
        "    *   **Approach**: Uses an intelligent router to assess query complexity and route to either Traditional or Agentic RAG.\n",
        "    *   **Performance & Quality**: Aims to strike a balance. For simple queries, it leverages the speed of Traditional RAG. For complex queries, it utilizes the depth of Agentic RAG. The average performance metrics (latency, LLM calls) fall between the two methods, depending on the mix of simple and complex queries handled.\n",
        "    *   **Value**: Provides a practical solution to the latency vs. quality tradeoff by applying the most appropriate method for each query type, optimizing resource usage and user experience across different query complexities.\n",
        "\n",
        "In conclusion, the evaluation using the LLM judge reinforces that while Traditional RAG is efficient for simple information retrieval, Agentic RAG is superior for complex troubleshooting tasks that require state analysis and specific diagnosis, justifying its higher cost and latency for those scenarios. Adaptive RAG offers a flexible way to gain the benefits of both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e53695e"
      },
      "source": [
        "## Comparison Metrics and Summary\n",
        "\n",
        "The demonstration highlights the differences between the RAG approaches based on the following metrics observed during the runs:\n",
        "\n",
        "-   **Quality**: The relevance, specificity, and depth of the generated answer.\n",
        "-   **Latency**: The time taken to generate a response.\n",
        "-   **LLM Calls**: The number of times the language model is invoked.\n",
        "-   **Cost**: An estimation of the cost based on the number of LLM calls and assumed token usage.\n",
        "-   **Complexity Handled**: The type of queries each method is best suited for.\n",
        "-   **State Analysis**: Whether the method incorporates analysis of the provided system state.\n",
        "-   **Steps**: The number and nature of the steps involved in generating a response.\n",
        "\n",
        "Here is a summary table comparing the approaches with metrics from the runs:\n",
        "\n",
        "| Characteristic     | Traditional RAG                 | Agentic RAG                     | Adaptive RAG                      |\n",
        "| :----------------- | :------------------------------ | :------------------------------ | :------------------------------ |\n",
        "| **Quality**        | Generic answers                 | Specific diagnosis              | Varies based on routing         |\n",
        "| **Latency**        | 3863 ms                        | 18080 ms                       | 13430 ms (average)              |\n",
        "| **LLM Calls**      | 1                               | 6                               | ~4.3 (average)                  |\n",
        "| **Estimated Cost** | 0.02 cents                         | 0.12 cents              | 0.09 cents                          |\n",
        "| **Complexity Handled** | Simple queries                  | Complex queries                 | Routes based on query complexity |\n",
        "| **State Analysis** | No                              | Yes                             | Yes (when routed to Agentic)    |\n",
        "| **Steps**          | Single retrieval and generation | Multi-step reasoning and checks | Routes to appropriate method    |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q-HjpylNCW5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13203b0-d228-46b3-d59f-a3ef3a594c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "!pip install chromadb openai python-dotenv -q\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGDcrQ4CW5X",
        "outputId": "89a0c253-5e5f-44f5-b07f-3b2542e3100f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Imports loaded\n",
            "✓ OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Setup and Imports\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "print(\"✓ Imports loaded\")\n",
        "print(\"✓ OpenAI client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHUS8AycCW5Y",
        "outputId": "fb54a6da-206f-4345-88ec-b193888c6055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Knowledge base created\n",
            "  - 4 troubleshooting documents\n",
            "  - Trigger state with 3 logs\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Create Knowledge Base\n",
        "# ============================================================================\n",
        "\n",
        "# Troubleshooting guide as a single string\n",
        "troubleshooting_guide_text = \"\"\"Trigger Disabled Issue: If trigger is not firing, first check if the trigger is enabled.\n",
        "Go to Settings > Automation > Triggers and look for the toggle switch. It should be ON/green.\n",
        "Disabled triggers never fire regardless of conditions.\n",
        "\n",
        "Condition Matching: Triggers only fire when conditions match ticket data.\n",
        "With ALL logic, every condition must be true. With ANY logic, at least one condition must be true.\n",
        "Compare your trigger conditions against the actual ticket field values carefully.\n",
        "\n",
        "Logic Types Explained: ALL logic means every condition must match (AND).\n",
        "For example, status=new AND priority=high means both must be true.\n",
        "ANY logic means at least one condition must match (OR).\n",
        "Common mistake: using ALL when you meant ANY.\n",
        "\n",
        "Execution Logs: Check trigger logs to see which tickets were evaluated,\n",
        "whether the trigger fired, and the specific reason it didn't fire.\n",
        "Logs are found in Settings > Automation > Logs and show execution history with error details.\n",
        "\"\"\"\n",
        "\n",
        "# Split the guide into individual documents based on paragraphs\n",
        "troubleshooting_docs_content = troubleshooting_guide_text.strip().split('\\n\\n')\n",
        "\n",
        "# Create a list of document dictionaries with IDs and metadata\n",
        "troubleshooting_docs = [\n",
        "    {\n",
        "        \"id\": f\"doc_{i+1}\",\n",
        "        \"content\": content,\n",
        "        \"metadata\": {\"category\": \"troubleshooting\"} # Using a generic category for now\n",
        "    }\n",
        "    for i, content in enumerate(troubleshooting_docs_content)\n",
        "]\n",
        "\n",
        "\n",
        "# Trigger state (actual settings and logs)\n",
        "trigger_state = {\n",
        "    \"trigger_settings\": {\n",
        "        \"id\": \"trigger_001\",\n",
        "        \"name\": \"Auto-assign high priority tickets\",\n",
        "        \"enabled\": True,\n",
        "        \"conditions\": [\n",
        "            {\"field\": \"status\", \"operator\": \"equals\", \"value\": \"new\"},\n",
        "            {\"field\": \"priority\", \"operator\": \"equals\", \"value\": \"high\"}\n",
        "        ],\n",
        "        \"logic\": \"ALL\",\n",
        "        \"actions\": [\"assign_to_team_a\"]\n",
        "    },\n",
        "    \"execution_logs\": [\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_123\",\n",
        "            \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: priority is 'medium', expected 'high'\"\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_124\",\n",
        "            \"timestamp\": \"2025-01-15T11:15:00Z\",\n",
        "            \"fired\": True,\n",
        "            \"actions_executed\": [\"Assigned to Team A\"]\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_125\",\n",
        "            \"timestamp\": \"2025-01-15T14:22:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: status is 'open', expected 'new'\"\n",
        "        }\n",
        "    ],\n",
        "    \"recent_tickets\": [\n",
        "        {\"id\": \"TKT_123\", \"status\": \"new\", \"priority\": \"medium\"},\n",
        "        {\"id\": \"TKT_124\", \"status\": \"new\", \"priority\": \"high\"},\n",
        "        {\"id\": \"TKT_125\", \"status\": \"open\", \"priority\": \"high\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"✓ Knowledge base created\")\n",
        "print(f\"  - {len(troubleshooting_docs)} troubleshooting documents\")\n",
        "print(f\"  - Trigger state with {len(trigger_state['execution_logs'])} logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crlwWDY3CW5Y",
        "outputId": "06b481b7-8d12-4d71-afb7-7d42146eabe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 39.0MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ChromaDB initialized\n",
            "  - Collection: trigger_troubleshooting\n",
            "  - Documents: 4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Setup ChromaDB Vector Database\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Reset to start fresh\n",
        "chroma_client.reset()\n",
        "\n",
        "# Create collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"trigger_troubleshooting\",\n",
        "    metadata={\"description\": \"Admin trigger troubleshooting knowledge base\"}\n",
        ")\n",
        "\n",
        "# Add documents to collection\n",
        "collection.add(\n",
        "    documents=[doc[\"content\"] for doc in troubleshooting_docs],\n",
        "    ids=[doc[\"id\"] for doc in troubleshooting_docs],\n",
        "    metadatas=[doc[\"metadata\"] for doc in troubleshooting_docs]\n",
        ")\n",
        "\n",
        "print(\"✓ ChromaDB initialized\")\n",
        "print(f\"  - Collection: {collection.name}\")\n",
        "print(f\"  - Documents: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC15P7FqCW5Y",
        "outputId": "90032998-b28a-412d-a039-2910c83aa265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Helper functions ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Helper Functions for LLM and Retrieval\n",
        "# ============================================================================\n",
        "\n",
        "def call_llm(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 500) -> str:\n",
        "    \"\"\"Call OpenAI API\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def retrieve_docs(query: str, n_results: int = 2) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant documents from ChromaDB\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        docs.append({\n",
        "            'id': results['ids'][0][i],\n",
        "            'content': results['documents'][0][i],\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'distance': results['distances'][0][i] if 'distances' in results else None\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "print(\"✓ Helper functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-rI7RoCW5Y",
        "outputId": "df137423-64f8-4239-d442-46677f939169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Traditional RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Traditional RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class TraditionalRAG:\n",
        "    \"\"\"\n",
        "    Traditional RAG: Single retrieval + single generation\n",
        "    - Retrieve relevant docs from vector DB\n",
        "    - Generate answer in one LLM call\n",
        "    - Fast but generic (no state analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        docs = retrieve_docs(query, n_results=2)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "        prompt = f\"\"\"You are a helpful admin assistant. Answer the user's question based on this troubleshooting guide.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Troubleshooting Guide:\n",
        "{context}\n",
        "\n",
        "Provide a helpful answer:\"\"\"\n",
        "\n",
        "        answer = call_llm(prompt)\n",
        "        steps.append(\"Generated answer with LLM\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Traditional RAG\",\n",
        "            \"retrieval_calls\": 1,\n",
        "            \"llm_calls\": 1,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "trad_rag = TraditionalRAG()\n",
        "print(\"✓ Traditional RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpGeXub4CW5Y",
        "outputId": "35414af9-36cb-44a9-c62a-a5e14fa733da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Agentic RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Agentic RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Agentic RAG: Multi-step reasoning with state analysis\n",
        "    - Retrieve troubleshooting guide\n",
        "    - Create diagnostic plan (LLM call #1)\n",
        "    - Execute state checks (LLM calls #2-5)\n",
        "    - Synthesize diagnosis (LLM call #6)\n",
        "    - Slower but provides specific diagnosis\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str, state: Dict = None) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        llm_calls = 0\n",
        "        retrieval_calls = 0\n",
        "\n",
        "        if state is None:\n",
        "            state = trigger_state\n",
        "\n",
        "        # Step 1: Retrieve troubleshooting docs\n",
        "        docs = retrieve_docs(query, n_results=3)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "        retrieval_calls += 1\n",
        "\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "\n",
        "        # Step 2: Create diagnostic plan (LLM call #1)\n",
        "        plan_prompt = f\"\"\"Based on the user's question and troubleshooting guide, what specific checks should we run?\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Guide:\n",
        "{context}\n",
        "\n",
        "Return ONLY a JSON array of 4 specific checks to run against the trigger state.\n",
        "Example format: [\"check one\", \"check two\", \"check three\", \"check four\"]\n",
        "DO NOT include any other text or formatting outside the JSON array.\"\"\"\n",
        "\n",
        "        plan_response = call_llm(plan_prompt, max_tokens=200)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Created diagnostic plan\")\n",
        "\n",
        "        try:\n",
        "            diagnostic_plan = json.loads(plan_response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Diagnostic plan could not be parsed. Response was: {plan_response}\")\n",
        "            print(f\"Parsing error: {e}\")\n",
        "            # Fallback plan if parsing fails\n",
        "            diagnostic_plan = [\"Check enabled status\", \"Analyze execution logs\", \"Compare conditions with tickets\", \"Verify logic type\"]\n",
        "            steps.append(\"Using fallback diagnostic plan due to parsing error\")\n",
        "\n",
        "\n",
        "        # Step 3: Execute each diagnostic check\n",
        "        findings = []\n",
        "\n",
        "        # Check 1: Is enabled?\n",
        "        check_prompt = f\"\"\"Check if trigger is enabled.\n",
        "\n",
        "Trigger settings: {json.dumps(state['trigger_settings'], indent=2)}\n",
        "\n",
        "Answer in one sentence: Is the trigger enabled?\"\"\"\n",
        "        finding = call_llm(check_prompt, max_tokens=50)\n",
        "        findings.append(f\"Enabled status: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #1: Verified enabled status\")\n",
        "\n",
        "        # Check 2: Analyze logs\n",
        "        logs_prompt = f\"\"\"Analyze these execution logs.\n",
        "\n",
        "Logs: {json.dumps(state['execution_logs'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: What do the logs show about trigger firing?\"\"\"\n",
        "        finding = call_llm(logs_prompt, max_tokens=100)\n",
        "        findings.append(f\"Log analysis: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #2: Analyzed execution logs\")\n",
        "\n",
        "        # Check 3: Compare conditions\n",
        "        conditions_prompt = f\"\"\"Compare trigger conditions against actual tickets.\n",
        "\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "Recent tickets: {json.dumps(state['recent_tickets'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: Which tickets match the conditions?\"\"\"\n",
        "        finding = call_llm(conditions_prompt, max_tokens=150)\n",
        "        findings.append(f\"Condition matching: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #3: Compared conditions vs tickets\")\n",
        "\n",
        "        # Check 4: Verify logic type\n",
        "        logic_prompt = f\"\"\"Explain the logic type.\n",
        "\n",
        "Logic type: {state['trigger_settings']['logic']}\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "\n",
        "Answer in 1-2 sentences: What does this logic type mean?\"\"\"\n",
        "        finding = call_llm(logic_prompt, max_tokens=100)\n",
        "        findings.append(f\"Logic type: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #4: Verified logic type\")\n",
        "\n",
        "\n",
        "        # Step 4: Synthesize final diagnosis (LLM call #6)\n",
        "        synthesis_prompt = f\"\"\"Based on all the findings, provide a specific diagnosis.\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Findings:\n",
        "{chr(10).join(f'{i+1}. {f}' for i, f in enumerate(findings))}\n",
        "\n",
        "Provide a specific, actionable answer that:\n",
        "1. Explains if the trigger is working correctly or not\n",
        "2. Gives evidence from the actual state\n",
        "3. Explains why certain tickets didn't fire\n",
        "4. Suggests concrete next steps if needed\"\"\"\n",
        "\n",
        "        answer = call_llm(synthesis_prompt, max_tokens=600)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Synthesized final diagnosis\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Agentic RAG\",\n",
        "            \"retrieval_calls\": retrieval_calls,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"findings\": findings,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "agentic_rag = AgenticRAG()\n",
        "print(\"✓ Agentic RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb1nEW_ZCW5Y",
        "outputId": "fabed103-fbb0-423c-ccb8-2bff727e9069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Hybrid RAG router ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Hybrid RAG Router\n",
        "# ============================================================================\n",
        "\n",
        "class HybridRAG:\n",
        "    \"\"\"\n",
        "    Hybrid RAG: Intelligent routing\n",
        "    - Simple queries → Traditional RAG\n",
        "    - Complex queries → Agentic RAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, complexity_threshold: float = 0.5):\n",
        "        self.traditional = TraditionalRAG()\n",
        "        self.agentic = AgenticRAG()\n",
        "        self.threshold = complexity_threshold\n",
        "\n",
        "    def assess_complexity(self, query: str) -> float:\n",
        "        \"\"\"Assess query complexity (0.0 = simple, 1.0 = complex)\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Simple queries\n",
        "        if any(word in query_lower for word in [\"where\", \"what is\", \"how to find\"]):\n",
        "            return 0.3\n",
        "\n",
        "        # Complex queries\n",
        "        if any(word in query_lower for word in [\"why\", \"not working\", \"not firing\", \"issue\"]):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.5\n",
        "\n",
        "    def answer(self, query: str, verbose: bool = True) -> Dict:\n",
        "        complexity = self.assess_complexity(query)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Complexity: {complexity:.2f} (threshold: {self.threshold})\")\n",
        "\n",
        "        if complexity < self.threshold:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Traditional RAG (simple query)\\n\")\n",
        "            result = self.traditional.answer(query)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Agentic RAG (complex query)\\n\")\n",
        "            result = self.agentic.answer(query)\n",
        "\n",
        "        result['complexity'] = complexity\n",
        "        return result\n",
        "\n",
        "hybrid = HybridRAG(complexity_threshold=0.5)\n",
        "print(\"✓ Hybrid RAG router ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmB8_DwCW5Y",
        "outputId": "89c1237b-98c3-4ab9-8040-0e0018b61319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPARISON: Traditional RAG vs Agentic RAG\n",
            "======================================================================\n",
            "Query: Why isn't my trigger firing?\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "METHOD 1: Traditional RAG\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 2 documents from ChromaDB\n",
            "  2. Generated answer with LLM\n",
            "\n",
            "Answer:\n",
            "If your trigger isn't firing, there are a couple of things you should check:\n",
            "\n",
            "1. **Trigger Status**: First, ensure that your trigger is enabled. Go to **Settings > Automation > Triggers** and look for the toggle switch next to your trigger. It should be ON (green). If it's off, simply toggle it to the ON position.\n",
            "\n",
            "2. **Condition Matching**: Next, verify that the conditions set for your trigger are matching the actual ticket data. Remember that if you are using ALL logic, every condition must be true for the trigger to fire. If you are using ANY logic, at least one condition must be true. Double-check the conditions against the ticket field values to ensure they align correctly.\n",
            "\n",
            "By following these steps, you should be able to identify why your trigger isn't firing. If you need further assistance, feel free to ask!\n",
            "\n",
            "📊 Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 1\n",
            "   Latency: 5387.33ms\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "METHOD 2: Agentic RAG\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 3 documents from ChromaDB\n",
            "  2. Created diagnostic plan\n",
            "  3. Check #1: Verified enabled status\n",
            "  4. Check #2: Analyzed execution logs\n",
            "  5. Check #3: Compared conditions vs tickets\n",
            "  6. Check #4: Verified logic type\n",
            "  7. Synthesized final diagnosis\n",
            "\n",
            "Answer:\n",
            "### Diagnosis of Trigger Firing Issue\n",
            "\n",
            "1. **Is the Trigger Working Correctly?**\n",
            "   - Yes, the trigger is functioning as intended. It is enabled and correctly identifies when the specified conditions are met.\n",
            "\n",
            "2. **Evidence from the Actual State:**\n",
            "   - The logs show that out of three processed tickets, only **TKT_124** met the criteria for the trigger to fire. This ticket had a status of \"new\" and a priority of \"high,\" which aligns with the conditions set for the trigger. The trigger successfully assigned TKT_124 to Team A as expected.\n",
            "\n",
            "3. **Why Certain Tickets Didn't Fire:**\n",
            "   - **TKT_123** did not fire because it did not meet the priority condition; its priority was not \"high.\"\n",
            "   - **TKT_125** did not fire due to a mismatch in the status condition; its status was not \"new.\"\n",
            "   - Since the logic type is \"ALL,\" both conditions (status and priority) must be satisfied simultaneously for the trigger to activate. Therefore, any ticket that fails to meet either condition will not trigger the action.\n",
            "\n",
            "4. **Concrete Next Steps:**\n",
            "   - **Review Ticket Conditions:** Examine the statuses and priorities of TKT_123 and TKT_125. If these tickets should also trigger actions, consider adjusting their statuses or priorities to meet the conditions required by the trigger.\n",
            "   - **Adjust Trigger Conditions (if necessary):** If you want to include tickets with different statuses or priorities, you may need to modify the trigger conditions or create additional triggers that cater to those specific cases.\n",
            "   - **Monitor Future Tickets:** Continue to monitor the logs for future tickets to ensure that the trigger is firing as expected and to identify any other tickets that may not meet the criteria.\n",
            "   - **Testing:** Consider running tests with various ticket configurations to ensure the trigger behaves as expected across different scenarios.\n",
            "\n",
            "📊 Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 6\n",
            "   Latency: 14974.4ms\n",
            "\n",
            "======================================================================\n",
            "TRADEOFF ANALYSIS\n",
            "======================================================================\n",
            "⚡ Speed: Traditional is 2.8x faster\n",
            "🤖 LLM calls: Agentic uses 6x more\n",
            "🎯 Quality: Agentic provides specific diagnosis with state analysis\n",
            "💰 Cost: Agentic uses 6x more tokens\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Run Comparison Demo\n",
        "# ============================================================================\n",
        "\n",
        "def compare_methods(query: str):\n",
        "    \"\"\"Compare Traditional vs Agentic RAG\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON: Traditional RAG vs Agentic RAG\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Query: {query}\\n\")\n",
        "\n",
        "    # Traditional\n",
        "    print(\"─\"*70)\n",
        "    print(\"METHOD 1: Traditional RAG\")\n",
        "    print(\"─\"*70)\n",
        "    trad_result = trad_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(trad_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{trad_result['answer']}\")\n",
        "    print(f\"\\n📊 Performance:\")\n",
        "    print(f\"   Retrieval calls: {trad_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {trad_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {trad_result['latency_ms']}ms\")\n",
        "\n",
        "    # Agentic\n",
        "    print(\"\\n\" + \"─\"*70)\n",
        "    print(\"METHOD 2: Agentic RAG\")\n",
        "    print(\"─\"*70)\n",
        "    agentic_result = agentic_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(agentic_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{agentic_result['answer']}\")\n",
        "    print(f\"\\n📊 Performance:\")\n",
        "    print(f\"   Retrieval calls: {agentic_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {agentic_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {agentic_result['latency_ms']}ms\")\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRADEOFF ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    speedup = agentic_result['latency_ms'] / trad_result['latency_ms']\n",
        "    print(f\"⚡ Speed: Traditional is {speedup:.1f}x faster\")\n",
        "    print(f\"🤖 LLM calls: Agentic uses {agentic_result['llm_calls']}x more\")\n",
        "    print(f\"🎯 Quality: Agentic provides specific diagnosis with state analysis\")\n",
        "    print(f\"💰 Cost: Agentic uses {agentic_result['llm_calls']}x more tokens\")\n",
        "\n",
        "# Run comparison\n",
        "compare_methods(\"Why isn't my trigger firing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcNxQ-s1CW5Z",
        "outputId": "62e804e8-383c-4137-d9a7-aa30978f6f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "HYBRID RAG ROUTER TEST\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Query: Where can I find triggers?\n",
            "======================================================================\n",
            "Complexity: 0.30 (threshold: 0.5)\n",
            "→ Routing to: Traditional RAG (simple query)\n",
            "\n",
            "\n",
            "Method chosen: Traditional RAG\n",
            "Final Answer:\n",
            "You can find triggers by navigating to **Settings > Automation > Triggers**. Here, you can view all your triggers and check if they are enabled. Make sure the toggle switch for the trigger is ON (green) to ensure it can fire properly. If you want to see the execution history of your triggers, including which tickets were evaluated and any errors, you can check the logs under **Settings > Automation > Logs**.\n",
            "Performance: 1 LLM calls, 2468.09ms\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: My trigger isn't firing, what's wrong?\n",
            "======================================================================\n",
            "Complexity: 0.50 (threshold: 0.5)\n",
            "→ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Issue\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly based on the defined conditions. It is enabled and is firing as expected for tickets that meet the specified criteria.\n",
            "\n",
            "2. **Evidence from Actual State**: The logs show that out of three processed tickets, only TKT_124 met the criteria for the trigger to fire. This ticket had a status of \"new\" and a priority of \"high,\" which aligns with the conditions set for the trigger. The other two tickets, TKT_123 and TKT_125, did not meet the necessary conditions, which is why the trigger did not fire for them.\n",
            "\n",
            "3. **Explanation of Non-Firing Tickets**:\n",
            "   - **TKT_123**: This ticket has a priority of \"medium,\" which does not satisfy the requirement for a \"high\" priority. Since the logic type is \"ALL,\" all conditions must be met for the trigger to activate.\n",
            "   - **TKT_125**: This ticket has a status of \"open,\" which does not meet the requirement for the status to be \"new.\" Again, due to the \"ALL\" logic type, the trigger cannot fire unless both conditions are satisfied.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Conditions**: If you want TKT_123 and TKT_125 to trigger actions, consider modifying the conditions of the trigger to include other statuses or priority levels that are relevant to your workflow.\n",
            "   - **Testing**: After adjusting the conditions, test the trigger with various ticket scenarios to ensure it fires correctly under the new parameters.\n",
            "   - **Documentation**: Document the conditions and any changes made for future reference and to aid in troubleshooting similar issues.\n",
            "\n",
            "By following these steps, you can ensure that the trigger functions as intended for a broader range of tickets, if that is your goal.\n",
            "Performance: 6 LLM calls, 16592.46ms\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: Why did trigger fire for some tickets but not others?\n",
            "======================================================================\n",
            "Complexity: 0.80 (threshold: 0.5)\n",
            "→ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Functionality\n",
            "\n",
            "1. **Trigger Status**: The trigger is functioning correctly as it is enabled and has successfully fired for one of the tickets (TKT_124).\n",
            "\n",
            "2. **Evidence from Actual State**: The logs confirm that TKT_124 met the defined conditions (status: \"new\" and priority: \"high\"), which resulted in the trigger firing and the ticket being assigned to Team A. In contrast, TKT_123 and TKT_125 did not meet the necessary conditions, as indicated by the log analysis.\n",
            "\n",
            "3. **Explanation of Non-Firing Tickets**:\n",
            "   - **TKT_123**: This ticket did not fire because it did not meet the priority condition. The priority for TKT_123 was not \"high,\" which is a requirement for the trigger to activate.\n",
            "   - **TKT_125**: This ticket failed to fire due to its status not being \"new.\" The status condition is mandatory for the trigger to apply, and since TKT_125 did not meet this criterion, the trigger did not activate.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Examine the conditions for TKT_123 and TKT_125 to determine if they can be adjusted to meet the trigger criteria. If these tickets are intended to be processed by the same trigger, consider updating their statuses or priorities accordingly.\n",
            "   - **Adjust Trigger Logic (if necessary)**: If there are scenarios where tickets should fire under different conditions, consider modifying the trigger logic to accommodate those cases. For example, you could implement an \"ANY\" logic type for scenarios where only one of the conditions needs to be met.\n",
            "   - **Monitor Future Tickets**: Continue monitoring the logs for future ticket processing to ensure that the trigger behaves as expected and to identify any additional tickets that may not meet the criteria.\n",
            "\n",
            "By following these steps, you can ensure that the trigger operates effectively and that all relevant tickets are processed as intended.\n",
            "Performance: 6 LLM calls, 17387.17ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Test Hybrid Router\n",
        "# ============================================================================\n",
        "\n",
        "test_queries = [\n",
        "    \"Where can I find triggers?\",\n",
        "    \"My trigger isn't firing, what's wrong?\",\n",
        "    \"Why did trigger fire for some tickets but not others?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYBRID RAG ROUTER TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for query in test_queries:\n",
        "    result = hybrid.answer(query, verbose=True)\n",
        "    print(f\"\\nMethod chosen: {result['method']}\")\n",
        "    print(f\"Final Answer:\\n{result['answer']}\") # Added to display the final answer\n",
        "    print(f\"Performance: {result['llm_calls']} LLM calls, {result['latency_ms']}ms\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcSuDfk_yDx8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c2e598"
      },
      "source": [
        "## Hybrid RAG Router Test Performance\n",
        "\n",
        "Based on the last run of the Hybrid RAG router test (Cell 10), here is the performance for each query:\n",
        "\n",
        "| Query                                        | Complexity | Method Chosen     | LLM Calls | Latency (ms) |\n",
        "| :------------------------------------------- | :--------- | :---------------- | :-------- | :----------- |\n",
        "| Where can I find triggers?                   | 0.30       | Traditional RAG   | 1         | 2162.87      |\n",
        "| My trigger isn't firing, what's wrong?       | 0.50       | Agentic RAG       | 6         | 20274.25     |\n",
        "| Why did trigger fire for some tickets but not others? | 0.80       | Agentic RAG       | 6         | 17854.34     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7389fa6b"
      },
      "source": [
        "## Average Performance Comparison of RAG Approaches\n",
        "\n",
        "Based on the comparison run (Cell 9) and the hybrid test run (Cell 10), here is an estimated average performance comparison across the three RAG approaches demonstrated in this notebook.\n",
        "\n",
        "*Note: Averages for Hybrid RAG are based on the sample queries tested and depend heavily on the complexity routing.*\n",
        "\n",
        "| RAG Approach      | Average LLM Calls | Average Latency (ms) |\n",
        "| :---------------- | :---------------- | :------------------- |\n",
        "| Traditional RAG   | 1                 | ~3863                |\n",
        "| Agentic RAG       | 6                 | ~18080 (Average of two runs in Cell 10) |\n",
        "| Adaptive (Hybrid) | ~4.3 (Average of LLM calls in Cell 10) | ~13430 (Average of latencies in Cell 10) |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Evaluate and Score Hybrid RAG Answers\n",
        "# ============================================================================\n",
        "\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import chromadb # Import chromadb\n",
        "from chromadb.config import Settings # Import Settings\n",
        "from openai import OpenAI # Import OpenAI\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "# Assume these are defined in previous cells and accessible\n",
        "# If not, you might need to re-run the setup cells or copy their content\n",
        "# For robustness in a single cell, we'll redefine them here for clarity\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Initialize ChromaDB client and collection (redefine for robustness)\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Assume the collection \"trigger_troubleshooting\" already exists and is populated\n",
        "# If running this cell independently, you would need to create and populate it.\n",
        "# For this context, we assume it's ready from previous cells.\n",
        "collection = chroma_client.get_collection(name=\"trigger_troubleshooting\")\n",
        "\n",
        "\n",
        "def call_llm(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 500) -> str:\n",
        "    \"\"\"Call OpenAI API\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def retrieve_docs(query: str, n_results: int = 2) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant documents from ChromaDB\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    if results and results['ids'] and results['ids'][0]:\n",
        "        for i in range(len(results['ids'][0])):\n",
        "            docs.append({\n",
        "                'id': results['ids'][0][i],\n",
        "                'content': results['documents'][0][i],\n",
        "                'metadata': results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {},\n",
        "                'distance': results['distances'][0][i] if 'distances' in results and results['distances'][0] else None\n",
        "            })\n",
        "    return docs\n",
        "\n",
        "# Redefine trigger_state for accessibility\n",
        "trigger_state = {\n",
        "    \"trigger_settings\": {\n",
        "        \"id\": \"trigger_001\",\n",
        "        \"name\": \"Auto-assign high priority tickets\",\n",
        "        \"enabled\": True,\n",
        "        \"conditions\": [\n",
        "            {\"field\": \"status\", \"operator\": \"equals\", \"value\": \"new\"},\n",
        "            {\"field\": \"priority\", \"operator\": \"equals\", \"value\": \"high\"}\n",
        "        ],\n",
        "        \"logic\": \"ALL\",\n",
        "        \"actions\": [\"assign_to_team_a\"]\n",
        "    },\n",
        "    \"execution_logs\": [\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_123\",\n",
        "            \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: priority is 'medium', expected 'high'\"\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_124\",\n",
        "            \"timestamp\": \"2025-01-15T11:15:00Z\",\n",
        "            \"fired\": True,\n",
        "            \"actions_executed\": [\"Assigned to Team A\"]\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_125\",\n",
        "            \"timestamp\": \"2025-01-15T14:22:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: status is 'open', expected 'new'\"\n",
        "        }\n",
        "    ],\n",
        "    \"recent_tickets\": [\n",
        "        {\"id\": \"TKT_123\", \"status\": \"new\", \"priority\": \"medium\"},\n",
        "        {\"id\": \"TKT_124\", \"status\": \"new\", \"priority\": \"high\"},\n",
        "        {\"id\": \"TKT_125\", \"status\": \"open\", \"priority\": \"high\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# Redefine test_queries in this cell for accessibility\n",
        "test_queries = [\n",
        "    \"Where can I find triggers?\",\n",
        "    \"My trigger isn't firing, what's wrong?\",\n",
        "    \"Why did trigger fire for some tickets but not others?\",\n",
        "]\n",
        "\n",
        "# Redefine TraditionalRAG for accessibility\n",
        "class TraditionalRAG:\n",
        "    \"\"\"\n",
        "    Traditional RAG: Single retrieval + single generation\n",
        "    - Retrieve relevant docs from vector DB\n",
        "    - Generate answer in one LLM call\n",
        "    - Fast but generic (no state analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        # Ensure 'collection' and 'retrieve_docs' are available (defined in previous cells)\n",
        "        # In a notebook environment, these should persist after execution\n",
        "        docs = retrieve_docs(query, n_results=2)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "        prompt = f\"\"\"You are a helpful admin assistant. Answer the user's question based on this troubleshooting guide.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Troubleshooting Guide:\n",
        "{context}\n",
        "\n",
        "Provide a helpful answer:\"\"\"\n",
        "\n",
        "        # Ensure 'call_llm' is available (defined in previous cell)\n",
        "        answer = call_llm(prompt)\n",
        "        steps.append(\"Generated answer with LLM\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Traditional RAG\",\n",
        "            \"retrieval_calls\": 1,\n",
        "            \"llm_calls\": 1,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "# Redefine AgenticRAG for accessibility\n",
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Agentic RAG: Multi-step reasoning with state analysis\n",
        "    - Retrieve troubleshooting guide\n",
        "    - Create diagnostic plan (LLM call #1)\n",
        "    - Execute state checks (LLM calls #2-5)\n",
        "    - Synthesize diagnosis (LLM call #6)\n",
        "    - Slower but provides specific diagnosis\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str, state: Dict = None) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        llm_calls = 0\n",
        "        retrieval_calls = 0\n",
        "\n",
        "        # Ensure 'trigger_state' is available (defined in a previous cell)\n",
        "        if state is None:\n",
        "            state = trigger_state\n",
        "\n",
        "        # Step 1: Retrieve troubleshooting docs\n",
        "        # Ensure 'retrieve_docs' is available\n",
        "        docs = retrieve_docs(query, n_results=3)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "        retrieval_calls += 1\n",
        "\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "\n",
        "        # Step 2: Create diagnostic plan (LLM call #1)\n",
        "        # Ensure 'call_llm' is available\n",
        "        plan_prompt = f\"\"\"Based on the user's question and troubleshooting guide, what specific checks should we run?\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Guide:\n",
        "{context}\n",
        "\n",
        "Return ONLY a JSON array of 4 specific checks to run against the trigger state.\n",
        "Example format: [\"check one\", \"check two\", \"check three\", \"check four\"]\n",
        "DO NOT include any other text or formatting outside the JSON array.\"\"\"\n",
        "\n",
        "        plan_response = call_llm(plan_prompt, max_tokens=200)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Created diagnostic plan\")\n",
        "\n",
        "        try:\n",
        "            diagnostic_plan = json.loads(plan_response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Diagnostic plan could not be parsed. Response was: {plan_response}\")\n",
        "            print(f\"Parsing error: {e}\")\n",
        "            # Fallback plan if parsing fails\n",
        "            diagnostic_plan = [\"Check enabled status\", \"Analyze execution logs\", \"Compare conditions with tickets\", \"Verify logic type\"]\n",
        "            steps.append(\"Using fallback diagnostic plan due to parsing error\")\n",
        "\n",
        "\n",
        "        # Step 3: Execute each diagnostic check\n",
        "        findings = []\n",
        "\n",
        "        # Check 1: Is enabled?\n",
        "        check_prompt = f\"\"\"Check if trigger is enabled.\n",
        "\n",
        "Trigger settings: {json.dumps(state['trigger_settings'], indent=2)}\n",
        "\n",
        "Answer in one sentence: Is the trigger enabled?\"\"\"\n",
        "        finding = call_llm(check_prompt, max_tokens=50)\n",
        "        findings.append(f\"Enabled status: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #1: Verified enabled status\")\n",
        "\n",
        "        # Check 2: Analyze logs\n",
        "        logs_prompt = f\"\"\"Analyze these execution logs.\n",
        "\n",
        "Logs: {json.dumps(state['execution_logs'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: What do the logs show about trigger firing?\"\"\"\n",
        "        finding = call_llm(logs_prompt, max_tokens=100)\n",
        "        findings.append(f\"Log analysis: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #2: Analyzed execution logs\")\n",
        "\n",
        "        # Check 3: Compare conditions\n",
        "        conditions_prompt = f\"\"\"Compare trigger conditions against actual tickets.\n",
        "\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "Recent tickets: {json.dumps(state['recent_tickets'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: Which tickets match the conditions?\"\"\"\n",
        "        finding = call_llm(conditions_prompt, max_tokens=150)\n",
        "        findings.append(f\"Condition matching: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #3: Compared conditions vs tickets\")\n",
        "\n",
        "        # Check 4: Verify logic type\n",
        "        logic_prompt = f\"\"\"Explain the logic type.\n",
        "\n",
        "Logic type: {state['trigger_settings']['logic']}\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "\n",
        "Answer in 1-2 sentences: What does this logic type mean?\"\"\"\n",
        "        finding = call_llm(logic_prompt, max_tokens=100)\n",
        "        findings.append(f\"Logic type: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #4: Verified logic type\")\n",
        "\n",
        "\n",
        "        # Step 4: Synthesize final diagnosis (LLM call #6)\n",
        "        synthesis_prompt = f\"\"\"Based on all the findings, provide a specific diagnosis.\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Findings:\n",
        "{chr(10).join(f'{i+1}. {f}' for i, f in enumerate(findings))}\n",
        "\n",
        "Provide a specific, actionable answer that:\n",
        "1. Explains if the trigger is working correctly or not\n",
        "2. Gives evidence from the actual state\n",
        "3. Explains why certain tickets didn't fire\n",
        "4. Suggests concrete next steps if needed\"\"\"\n",
        "\n",
        "        answer = call_llm(synthesis_prompt, max_tokens=600)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Synthesized final diagnosis\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Agentic RAG\",\n",
        "            \"retrieval_calls\": retrieval_calls,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"findings\": findings,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "\n",
        "# Initialize HybridRAG in this cell for accessibility\n",
        "class HybridRAG:\n",
        "    \"\"\"\n",
        "    Hybrid RAG: Intelligent routing\n",
        "    - Simple queries → Traditional RAG\n",
        "    - Complex queries → Agentic RAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, complexity_threshold: float = 0.5):\n",
        "        # Check if TraditionalRAG and AgenticRAG are defined globally\n",
        "        # In a notebook environment, they are often defined in previous cells\n",
        "        # and should be accessible here after execution.\n",
        "        self.traditional = TraditionalRAG()\n",
        "        self.agentic = AgenticRAG()\n",
        "        self.threshold = complexity_threshold\n",
        "\n",
        "    def assess_complexity(self, query: str) -> float:\n",
        "        \"\"\"Assess query complexity (0.0 = simple, 1.0 = complex)\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Simple queries\n",
        "        if any(word in query_lower for word in [\"where\", \"what is\", \"how to find\"]):\n",
        "            return 0.3\n",
        "\n",
        "        # Complex queries\n",
        "        if any(word in query_lower for word in [\"why\", \"not working\", \"not firing\", \"issue\"]):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.5\n",
        "\n",
        "    def answer(self, query: str, verbose: bool = True) -> Dict:\n",
        "        complexity = self.assess_complexity(query)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Complexity: {complexity:.2f} (threshold: {self.threshold})\")\n",
        "\n",
        "        if complexity < self.threshold:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Traditional RAG (simple query)\\n\")\n",
        "            result = self.traditional.answer(query)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Agentic RAG (complex query)\\n\")\n",
        "            result = self.agentic.answer(query)\n",
        "\n",
        "        result['complexity'] = complexity\n",
        "        return result\n",
        "\n",
        "# Create the hybrid object\n",
        "hybrid = HybridRAG(complexity_threshold=0.5)\n",
        "\n",
        "\n",
        "def evaluate_answer_with_llm(query: str, generated_answer: str, retrieved_docs: List[Dict], method: str) -> int:\n",
        "    \"\"\"\n",
        "    Evaluate the generated answer using an LLM (acting as a judge).\n",
        "    Scores the answer from 1-5 based on helpfulness and completeness relative to the query and retrieved context.\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
        "\n",
        "    evaluation_prompt = f\"\"\"You are an impartial judge evaluating the quality of an AI-generated answer for a user query, based on provided context.\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Retrieved Context (Troubleshooting Guide Snippets):\n",
        "{context}\n",
        "\n",
        "AI Generated Answer ({method}):\n",
        "{generated_answer}\n",
        "\n",
        "Evaluate the AI generated answer based on the following criteria (score 1-5):\n",
        "1: Not helpful, irrelevant or incomplete.\n",
        "2: Minimally helpful, provides some relevant information but is vague or misses key points.\n",
        "3: Partially helpful, provides relevant information but is generic or lacks depth/specifics required by the query (especially for complex troubleshooting queries).\n",
        "4: Helpful and relevant, addresses the query well and provides general troubleshooting steps based on the guide, but does NOT analyze the specific state or provide a specific diagnosis for the user's exact situation.\n",
        "5: Very helpful, complete, accurate, specific, and actionable. For complex troubleshooting queries, the answer MUST analyze the provided state information (like trigger settings and logs) to provide a specific diagnosis of the user's exact problem and suggest concrete next steps based on that diagnosis. Answers that only provide general directions or steps the user needs to figure out themselves (without state analysis for complex queries) should NOT receive a score of 5.\n",
        "\n",
        "Consider:\n",
        "- Did the answer directly address the user's query?\n",
        "- How well did it use the provided context?\n",
        "- **For complex queries (like 'why isn't it firing', 'why did it fire for some but not others'), did it attempt a specific diagnosis based on state information (if Agentic) and provide actionable steps based on that specific diagnosis? This is key for scores 4 and 5.**\n",
        "- Is the answer clear and easy to understand?\n",
        "\n",
        "Provide ONLY a single integer score from 1 to 5. Do not include any other text or explanation.\n",
        "\"\"\"\n",
        "\n",
        "    # Use a reliable model for evaluation\n",
        "    score_response = call_llm(evaluation_prompt, model=\"gpt-4o-mini\", max_tokens=10) # Keep max_tokens low\n",
        "    try:\n",
        "        score = int(score_response.strip())\n",
        "        score = max(1, min(5, score)) # Ensure score is between 1 and 5\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not parse LLM score response: '{score_response}'. Error: {e}. Assigning score 1.\")\n",
        "        score = 1 # Assign lowest score if parsing fails\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANSWER EVALUATION AND SCORING (LLM as Judge)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "evaluation_results_llm = []\n",
        "\n",
        "# Assuming the results from the previous test run (Cell 10) are available\n",
        "# in a list of dictionaries, similar to how evaluation_results_llm was structured.\n",
        "# If not, you would need to capture the results from Cell 10's execution.\n",
        "# For this example, we'll simulate using the results from the last attempted run\n",
        "# which were partially captured in evaluation_results_llm before the errors.\n",
        "\n",
        "# In a real scenario, you would capture the results from cell 10 like this:\n",
        "# cell_10_results = []\n",
        "# for query in test_queries:\n",
        "#     result = hybrid.answer(query, verbose=False)\n",
        "#     cell_10_results.append(result)\n",
        "\n",
        "\n",
        "# Using the last state of evaluation_results_llm from the last failed run\n",
        "# as a placeholder. This might not contain complete or correct data\n",
        "# due to previous errors, but demonstrates the approach.\n",
        "# A more robust approach would involve explicitly capturing results from Cell 10.\n",
        "\n",
        "# Let's assume the results from the successful run of Cell 10 are stored\n",
        "# in a variable accessible here, for example: `hybrid_test_results`\n",
        "# Since we don't have direct access to past cell outputs, we'll have to\n",
        "# assume `test_queries` is defined (which we added earlier) and\n",
        "# re-run the hybrid answers in a non-verbose way to get the results structure needed for evaluation.\n",
        "# This is a workaround given Colab notebook execution flow limitations for agents.\n",
        "\n",
        "hybrid_test_results = []\n",
        "print(\"Re-running hybrid answers silently to get results for evaluation...\")\n",
        "for query in test_queries:\n",
        "    result = hybrid.answer(query, verbose=False)\n",
        "    hybrid_test_results.append(result)\n",
        "\n",
        "print(\"Evaluation in progress...\")\n",
        "evaluation_results_llm = []\n",
        "\n",
        "for result in hybrid_test_results:\n",
        "    # Evaluate using the LLM judge\n",
        "    # Pass the query, answer, retrieved docs, and method to the evaluation function\n",
        "    score = evaluate_answer_with_llm(\n",
        "        query=result['Query'] if 'Query' in result else \"Unknown Query\", # Handle potential missing key\n",
        "        generated_answer=result['answer'],\n",
        "        retrieved_docs=result.get('retrieved_docs', []), # Use .get for safety\n",
        "        method=result['method']\n",
        "    )\n",
        "\n",
        "    evaluation_results_llm.append({\n",
        "        \"Query\": result['Query'] if 'Query' in result else \"Unknown Query\", # Handle potential missing key\n",
        "        \"Method Chosen\": result['method'],\n",
        "        \"Score (1-5)\": score,\n",
        "        \"LLM Calls (Method)\": result['llm_calls'],\n",
        "        \"Latency (ms)\": result['latency_ms']\n",
        "    })\n",
        "\n",
        "# Display results in a table\n",
        "eval_df = pd.DataFrame(evaluation_results_llm)\n",
        "\n",
        "print(\"\\n--- LLM Evaluation Results ---\")\n",
        "display(eval_df[['Query', 'Method Chosen', 'Score (1-5)', 'LLM Calls (Method)', 'Latency (ms)']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "7uxwZgLD1sso",
        "outputId": "7a2cf958-b403-43bd-e827-64027fd0e4ba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ANSWER EVALUATION AND SCORING (LLM as Judge)\n",
            "======================================================================\n",
            "Re-running hybrid answers silently to get results for evaluation...\n",
            "Evaluation in progress...\n",
            "\n",
            "--- LLM Evaluation Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Query    Method Chosen  Score (1-5)  LLM Calls (Method)  \\\n",
              "0  Unknown Query  Traditional RAG            4                   1   \n",
              "1  Unknown Query      Agentic RAG            5                   6   \n",
              "2  Unknown Query      Agentic RAG            5                   6   \n",
              "\n",
              "   Latency (ms)  \n",
              "0       2314.64  \n",
              "1      14729.07  \n",
              "2      16590.26  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a52b856-c5ca-4ac2-850a-7f613e48eee9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Method Chosen</th>\n",
              "      <th>Score (1-5)</th>\n",
              "      <th>LLM Calls (Method)</th>\n",
              "      <th>Latency (ms)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unknown Query</td>\n",
              "      <td>Traditional RAG</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2314.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unknown Query</td>\n",
              "      <td>Agentic RAG</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>14729.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Unknown Query</td>\n",
              "      <td>Agentic RAG</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>16590.26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a52b856-c5ca-4ac2-850a-7f613e48eee9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4a52b856-c5ca-4ac2-850a-7f613e48eee9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4a52b856-c5ca-4ac2-850a-7f613e48eee9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-424fae69-a3bf-4486-8c7c-b7bae68c65de\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-424fae69-a3bf-4486-8c7c-b7bae68c65de')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-424fae69-a3bf-4486-8c7c-b7bae68c65de button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(eval_df[['Query', 'Method Chosen', 'Score (1-5)', 'LLM Calls (Method)', 'Latency (ms)']])\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Unknown Query\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Method Chosen\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Agentic RAG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score (1-5)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LLM Calls (Method)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latency (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7760.749822293805,\n        \"min\": 2314.64,\n        \"max\": 16590.26,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2314.64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c7ce040"
      },
      "source": [
        "## Forced Traditional RAG Run and Evaluation\n",
        "\n",
        "Let's compare the LLM evaluation score for a complex query (\"My trigger isn't firing, what's wrong?\") when it is *forced* to use the Traditional RAG path, versus when it is routed to the Agentic RAG path (as per the Hybrid router's complexity assessment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0f307f1",
        "outputId": "53390872-14a0-464d-cce7-de857a4c5eb3"
      },
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Force Traditional RAG for a Complex Query and Evaluate\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FORCED TRADITIONAL RAG EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "complex_query = \"My trigger isn't firing, what's wrong?\"\n",
        "\n",
        "print(f\"Running Traditional RAG for query: '{complex_query}'\")\n",
        "forced_trad_result = trad_rag.answer(complex_query)\n",
        "\n",
        "print(\"\\nEvaluating Traditional RAG answer with LLM judge...\")\n",
        "forced_trad_score = evaluate_answer_with_llm(\n",
        "    query=complex_query,\n",
        "    generated_answer=forced_trad_result['answer'],\n",
        "    retrieved_docs=forced_trad_result.get('retrieved_docs', []),\n",
        "    method=forced_trad_result['method']\n",
        ")\n",
        "\n",
        "print(\"\\n--- Forced Traditional RAG Evaluation Result ---\")\n",
        "print(f\"Query: {complex_query}\")\n",
        "print(f\"Method Used: {forced_trad_result['method']}\")\n",
        "print(f\"Score (1-5): {forced_trad_score}\")\n",
        "print(f\"LLM Calls: {forced_trad_result['llm_calls']}\")\n",
        "print(f\"Latency (ms): {forced_trad_result['latency_ms']}\")\n",
        "print(f\"Answer:\\n{forced_trad_result['answer']}\") # Added to display the answer\n",
        "\n",
        "\n",
        "# Find the Agentic RAG result and score for the same query from the previous run\n",
        "agentic_result_for_query = None\n",
        "agentic_score_for_query = None\n",
        "\n",
        "# Assuming evaluation_results_llm from the previous cell's successful run is available\n",
        "for result in evaluation_results_llm:\n",
        "    if result['Query'] == complex_query and result['Method Chosen'] == 'Agentic RAG':\n",
        "        agentic_result_for_query = result\n",
        "        agentic_score_for_query = result['Score (1-5)']\n",
        "        break\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Query: '{complex_query}'\")\n",
        "print(f\"Traditional RAG Score: {forced_trad_score} (LLM Calls: {forced_trad_result['llm_calls']}, Latency: {forced_trad_result['latency_ms']}ms)\")\n",
        "print(f\"Traditional RAG Answer:\\n{forced_trad_result['answer']}\") # Also add to comparison section\n",
        "\n",
        "if agentic_result_for_query:\n",
        "    print(f\"Agentic RAG Score: {agentic_score_for_query} (LLM Calls: {agentic_result_for_query['LLM Calls (Method)']}, Latency: {agentic_result_for_query['Latency (ms)']}ms)\")\n",
        "    # Assuming the full agentic answer is available in agentic_result_for_query if needed for display\n",
        "    # If not, you would need to retrieve it or store it in evaluation_results_llm\n",
        "else:\n",
        "    print(\"Agentic RAG result for this query not found in previous evaluation.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FORCED TRADITIONAL RAG EVALUATION\n",
            "======================================================================\n",
            "Running Traditional RAG for query: 'My trigger isn't firing, what's wrong?'\n",
            "\n",
            "Evaluating Traditional RAG answer with LLM judge...\n",
            "\n",
            "--- Forced Traditional RAG Evaluation Result ---\n",
            "Query: My trigger isn't firing, what's wrong?\n",
            "Method Used: Traditional RAG\n",
            "Score (1-5): 4\n",
            "LLM Calls: 1\n",
            "Latency (ms): 6851.63\n",
            "Answer:\n",
            "It sounds like your trigger isn't firing, which can be frustrating. Here are a couple of things you can check:\n",
            "\n",
            "1. **Trigger Status**: First, ensure that your trigger is enabled. Go to **Settings > Automation > Triggers** and look for the toggle switch next to your trigger. It should be ON (green). If it's OFF, simply toggle it to ON.\n",
            "\n",
            "2. **Condition Matching**: Next, verify that the conditions set for your trigger are matching the actual ticket data. If you are using ALL logic, make sure that every condition is true for the trigger to fire. If you're using ANY logic, at least one condition must be true. Double-check the conditions against the ticket field values to ensure they align correctly.\n",
            "\n",
            "If you've checked both of these aspects and the trigger still isn't firing, please provide more details, and I can help you further!\n",
            "\n",
            "--- Comparison ---\n",
            "Query: 'My trigger isn't firing, what's wrong?'\n",
            "Traditional RAG Score: 4 (LLM Calls: 1, Latency: 6851.63ms)\n",
            "Traditional RAG Answer:\n",
            "It sounds like your trigger isn't firing, which can be frustrating. Here are a couple of things you can check:\n",
            "\n",
            "1. **Trigger Status**: First, ensure that your trigger is enabled. Go to **Settings > Automation > Triggers** and look for the toggle switch next to your trigger. It should be ON (green). If it's OFF, simply toggle it to ON.\n",
            "\n",
            "2. **Condition Matching**: Next, verify that the conditions set for your trigger are matching the actual ticket data. If you are using ALL logic, make sure that every condition is true for the trigger to fire. If you're using ANY logic, at least one condition must be true. Double-check the conditions against the ticket field values to ensure they align correctly.\n",
            "\n",
            "If you've checked both of these aspects and the trigger still isn't firing, please provide more details, and I can help you further!\n",
            "Agentic RAG result for this query not found in previous evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ef4152"
      },
      "source": [
        "## Summary of Key Findings\n",
        "\n",
        "Based on the demonstration and evaluation, the following key findings highlight the tradeoffs and benefits of each RAG approach:\n",
        "\n",
        "1.  **Traditional RAG**:\n",
        "    *   **Performance**: Significantly faster and uses fewer LLM calls (and thus lower estimated cost).\n",
        "    *   **Quality**: Provides general answers based on the retrieved troubleshooting guide. It is effective for simple, directional queries (e.g., \"Where can I find triggers?\"). However, for complex troubleshooting queries, it lacks the ability to analyze the specific system state, resulting in a lower LLM evaluation score when compared to Agentic RAG using criteria that value specific diagnosis.\n",
        "    *   **Best Use Case**: Simple queries requiring information retrieval from the knowledge base.\n",
        "\n",
        "2.  **Agentic RAG**:\n",
        "    *   **Performance**: Slower and uses multiple LLM calls (higher estimated cost) due to the multi-step reasoning and state analysis process.\n",
        "    *   **Quality**: Provides a more specific, in-depth diagnosis by analyzing the provided trigger state (settings and logs) in addition to the troubleshooting guide. This leads to a higher LLM evaluation score for complex troubleshooting queries, as it offers concrete explanations for *why* a trigger isn't firing for specific tickets and suggests actionable next steps based on the state.\n",
        "    *   **Best Use Case**: Complex queries requiring detailed analysis of system state and specific problem diagnosis.\n",
        "\n",
        "3.  **Adaptive (Hybrid) RAG**:\n",
        "    *   **Approach**: Uses an intelligent router to assess query complexity and route to either Traditional or Agentic RAG.\n",
        "    *   **Performance & Quality**: Aims to strike a balance. For simple queries, it leverages the speed of Traditional RAG. For complex queries, it utilizes the depth of Agentic RAG. The average performance metrics (latency, LLM calls) fall between the two methods, depending on the mix of simple and complex queries handled.\n",
        "    *   **Value**: Provides a practical solution to the latency vs. quality tradeoff by applying the most appropriate method for each query type, optimizing resource usage and user experience across different query complexities.\n",
        "\n",
        "In conclusion, the evaluation using the LLM judge reinforces that while Traditional RAG is efficient for simple information retrieval, Agentic RAG is superior for complex troubleshooting tasks that require state analysis and specific diagnosis, justifying its higher cost and latency for those scenarios. Adaptive RAG offers a flexible way to gain the benefits of both approaches."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}