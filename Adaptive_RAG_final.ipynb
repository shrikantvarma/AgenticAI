{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrikantvarma/AgenticAI/blob/main/Adaptive_RAG_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d9e8d50"
      },
      "source": [
        "## Summary of Learnings between three RAG approaches\n",
        "\n",
        "This notebook demonstrated and evaluated three approaches to RAG: Traditional, Agentic, and Adaptive (Hybrid). The key learnings regarding their performance and quality tradeoffs are summarized below, including observed metrics from our experiment:\n",
        "\n",
        "**Observed Performance and Quality Metrics:**\n",
        "\n",
        "| Metric            | Traditional RAG (Simple Query Example) | Agentic RAG (Complex Query Example) | Adaptive (Hybrid) RAG (Average/Routing) |\n",
        "| :---------------- | :------------------------------------- | :---------------------------------- | :-------------------------------------- |\n",
        "| LLM Score (1-5)   | 4                                      | 5                                   | Varies by query complexity (4 for simple, 5 for complex) |\n",
        "| Latency (ms)      | ~3400 (e.g., \"Where can I find triggers?\") | ~18000 - 21000 (e.g., \"My trigger isn't firing...\") | Depends on routing (faster for simple, slower for complex) |\n",
        "| LLM Calls         | 1                                      | 6                                   | 1 (for simple) or 6 (for complex)       |\n",
        "| Total Tokens      | ~230                                   | ~1700 - 1750                        | ~230 (for simple) or ~1700-1750 (for complex) |\n",
        "\n",
        "*Note: The exact numbers in the table are illustrative based on typical runs observed in this notebook. Adaptive RAG metrics depend on the router's decision.*\n",
        "\n",
        "**Traditional RAG:**\n",
        "*   **Quality:** Provides general, directional answers based on retrieved documents. Effective for simple queries but lacks depth and specific diagnosis for complex troubleshooting. Achieved lower scores from the LLM judge on complex queries compared to Agentic RAG.\n",
        "*   **Latency:** Significantly faster due to a single retrieval and single LLM call.\n",
        "*   **Token Usage (Cost):** Uses the fewest tokens per query, resulting in lower estimated cost.\n",
        "\n",
        "**Agentic RAG:**\n",
        "*   **Quality:** Provides specific, in-depth diagnoses and actionable steps by analyzing system state (settings and logs) alongside retrieved documents. Achieved higher scores from the LLM judge on complex troubleshooting queries.\n",
        "*   **Latency:** Slower due to multiple LLM calls and sequential reasoning steps.\n",
        "*   **Token Usage (Cost):** Uses significantly more tokens per query due to multiple LLM interactions, resulting in higher estimated cost.\n",
        "\n",
        "**Adaptive (Hybrid) RAG:**\n",
        "*   **Quality:** Selects the appropriate RAG method based on query complexity, aiming to deliver the best quality for each type (specific diagnosis for complex, general info for simple).\n",
        "*   **Latency:** Balances speed by using the faster Traditional RAG for simple queries and accepting the higher latency of Agentic RAG only when necessary for complex queries. Overall latency depends on the mix of query types.\n",
        "*   **Token Usage (Cost):** Balances cost by using the lower-token Traditional RAG for simple queries and incurring the higher token cost of Agentic RAG only for complex queries. Overall token usage depends on the mix of query types.\n",
        "\n",
        "**Overall Learning:**\n",
        "The choice of RAG architecture involves a clear tradeoff between speed/cost and the depth/specificity of the answer. Adaptive RAG provides a practical solution by intelligently routing queries to leverage the strengths of both Traditional and Agentic approaches, optimizing resource usage and user experience across varying query complexities. For real-world applications, understanding the nature of user queries is crucial for setting the complexity threshold and fine-tuning the router."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221d9f62"
      },
      "source": [
        "# The Experiemnt : Traditional, Agentic RAG , Adaptive/Hybrid RAG\n",
        "\n",
        "**A Practical Comparison with ChromaDB and OpenAI**\n",
        "\n",
        "This notebook explores **Adaptive/Hybrid Retrieval-Augmented Generation (RAG)**, demonstrating how it can intelligently combine the strengths of **Traditional RAG** and **Agentic RAG** approaches. By routing queries based on their complexity, Adaptive RAG aims to optimize the balance between **latency** (speed) and **quality** (accuracy and depth of analysis) for different types of user questions.\n",
        "\n",
        "We use an **Admin trigger troubleshooting use case** to illustrate these concepts, leveraging:\n",
        "- **ChromaDB**: A vector database for efficient storage and retrieval of troubleshooting knowledge.\n",
        "- **OpenAI GPT-4o-mini**: A powerful language model for generating responses and executing diagnostic steps.\n",
        "\n",
        "The scenario is as follows:\n",
        "1. There is a trigger in the system that failed.\n",
        "2. There is a document that explains how to troubleshoot triggers.\n",
        "3. The trigger setting and logs are available to the LLM.\n",
        "4. We use LLMs to provide answers.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Understanding the RAG Approaches\n",
        "\n",
        "- **Traditional RAG**:\n",
        "  - **Process**: Simple, single-step approach. Retrieves relevant documents from a vector database based on the user's query and uses an LLM to generate an answer based *only* on the retrieved context from the troubleshooting guide. **It does not analyze the specific trigger settings or logs.**\n",
        "  - **Characteristics**: Generally **fast** and **low cost** due to minimal LLM interaction. Provides **generic answers** without analyzing specific system states.\n",
        "\n",
        "- **Agentic RAG**:\n",
        "  - **Process**: A multi-step reasoning process that involves multiple LLM calls. It retrieves relevant information from the troubleshooting guides, creates a troubleshooting plan dynamically, *and* analyzes provided state data (like trigger settings and logs) to form a diagnosis. When a user submits a query, relevant chunks are retrieved from the vector database. This is then sent to the LLM with a request to create a dynamic plan based on the troubleshooting guide. The agent then executes each troubleshooting step with the help of the LLM, analyzing the provided trigger settings and logs for each step, and provides the result and suggestion from each execution before synthesizing a final answer.\n",
        "  - **Characteristics**: **Thorough** and provides a **specific solution** by analyzing the actual system state. However, it is **slower** and **more costly** due to the increased number of steps and LLM interactions.\n",
        "\n",
        "- **Adaptive RAG**:\n",
        "  - **Process**: Introduces an intelligent router that assesses the **complexity** of the incoming user query. Simple queries are directed to the faster Traditional RAG path, while complex queries requiring deeper analysis are sent to the more thorough Agentic RAG path.\n",
        "  - **Characteristics**: Aims to achieve the **best balance** between speed and quality by using the most appropriate method for each query type.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Demonstration and Comparison\n",
        "\n",
        "The notebook demonstrates these approaches by:\n",
        "1. Setting up a knowledge base in ChromaDB and simulating trigger state data.\n",
        "2. Implementing and running both Traditional and Agentic RAG methods on a troubleshooting query.\n",
        "3. Comparing their performance metrics (Latency, LLM Calls) and output quality, **including evaluation of output quality using an LLM as a judge.**\n",
        "4. Implementing a simple Hybrid RAG router and testing how it routes different types of queries.\n",
        "\n",
        "* * *\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- No single RAG method is ideal for all situations; there's an inherent **latency vs quality tradeoff**.\n",
        "- Traditional RAG is effective for quick information retrieval (simple queries).\n",
        "- Agentic RAG is powerful for complex problem-solving requiring state analysis.\n",
        "- **Hybrid RAG** provides a practical solution to this tradeoff by dynamically choosing the optimal approach per query.\n",
        "- Effective **query complexity assessment** is fundamental to a successful Hybrid RAG implementation.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Q-HjpylNCW5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3364d56b-edce-41b3-bdc4-ee2b368e4f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "!pip install chromadb openai python-dotenv -q\n",
        "\n",
        "print(\"âœ“ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGDcrQ4CW5X",
        "outputId": "06f18e05-1f50-4bfc-8e38-dd097946f60d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Imports loaded\n",
            "âœ“ OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Setup and Imports\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "print(\"âœ“ Imports loaded\")\n",
        "print(\"âœ“ OpenAI client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHUS8AycCW5Y",
        "outputId": "b613015d-f134-41c1-e537-1558f92adb0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Knowledge base created\n",
            "  - 4 troubleshooting documents\n",
            "  - Trigger state with 3 logs\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Create Knowledge Base\n",
        "# ============================================================================\n",
        "\n",
        "# Troubleshooting guide as a single string\n",
        "troubleshooting_guide_text = \"\"\"Trigger Disabled Issue: If trigger is not firing, first check if the trigger is enabled.\n",
        "Go to Settings > Automation > Triggers and look for the toggle switch. It should be ON/green.\n",
        "Disabled triggers never fire regardless of conditions.\n",
        "\n",
        "Condition Matching: Triggers only fire when conditions match ticket data.\n",
        "With ALL logic, every condition must be true. With ANY logic, at least one condition must be true.\n",
        "Compare your trigger conditions against the actual ticket field values carefully.\n",
        "\n",
        "Logic Types Explained: ALL logic means every condition must match (AND).\n",
        "For example, status=new AND priority=high means both must be true.\n",
        "ANY logic means at least one condition must match (OR).\n",
        "Common mistake: using ALL when you meant ANY.\n",
        "\n",
        "Execution Logs: Check trigger logs to see which tickets were evaluated,\n",
        "whether the trigger fired, and the specific reason it didn't fire.\n",
        "Logs are found in Settings > Automation > Logs and show execution history with error details.\n",
        "\"\"\"\n",
        "\n",
        "# Split the guide into individual documents based on paragraphs\n",
        "troubleshooting_docs_content = troubleshooting_guide_text.strip().split('\\n\\n')\n",
        "\n",
        "# Create a list of document dictionaries with IDs and metadata\n",
        "troubleshooting_docs = [\n",
        "    {\n",
        "        \"id\": f\"doc_{i+1}\",\n",
        "        \"content\": content,\n",
        "        \"metadata\": {\"category\": \"troubleshooting\"} # Using a generic category for now\n",
        "    }\n",
        "    for i, content in enumerate(troubleshooting_docs_content)\n",
        "]\n",
        "\n",
        "\n",
        "# Trigger state (actual settings and logs)\n",
        "trigger_state = {\n",
        "    \"trigger_settings\": {\n",
        "        \"id\": \"trigger_001\",\n",
        "        \"name\": \"Auto-assign high priority tickets\",\n",
        "        \"enabled\": True,\n",
        "        \"conditions\": [\n",
        "            {\"field\": \"status\", \"operator\": \"equals\", \"value\": \"new\"},\n",
        "            {\"field\": \"priority\", \"operator\": \"equals\", \"value\": \"high\"}\n",
        "        ],\n",
        "        \"logic\": \"ALL\",\n",
        "        \"actions\": [\"assign_to_team_a\"]\n",
        "    },\n",
        "    \"execution_logs\": [\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_123\",\n",
        "            \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: priority is 'medium', expected 'high'\"\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_124\",\n",
        "            \"timestamp\": \"2025-01-15T11:15:00Z\",\n",
        "            \"fired\": True,\n",
        "            \"actions_executed\": [\"Assigned to Team A\"]\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_125\",\n",
        "            \"timestamp\": \"2025-01-15T14:22:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: status is 'open', expected 'new'\"\n",
        "        }\n",
        "    ],\n",
        "    \"recent_tickets\": [\n",
        "        {\"id\": \"TKT_123\", \"status\": \"new\", \"priority\": \"medium\"},\n",
        "        {\"id\": \"TKT_124\", \"status\": \"new\", \"priority\": \"high\"},\n",
        "        {\"id\": \"TKT_125\", \"status\": \"open\", \"priority\": \"high\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"âœ“ Knowledge base created\")\n",
        "print(f\"  - {len(troubleshooting_docs)} troubleshooting documents\")\n",
        "print(f\"  - Trigger state with {len(trigger_state['execution_logs'])} logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crlwWDY3CW5Y",
        "outputId": "b1321ffb-6aeb-4ae7-fd7b-3eb638ef442b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ ChromaDB initialized\n",
            "  - Collection: trigger_troubleshooting\n",
            "  - Documents: 4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Setup ChromaDB Vector Database\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Reset to start fresh\n",
        "chroma_client.reset()\n",
        "\n",
        "# Create collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"trigger_troubleshooting\",\n",
        "    metadata={\"description\": \"Admin trigger troubleshooting knowledge base\"}\n",
        ")\n",
        "\n",
        "# Add documents to collection\n",
        "collection.add(\n",
        "    documents=[doc[\"content\"] for doc in troubleshooting_docs],\n",
        "    ids=[doc[\"id\"] for doc in troubleshooting_docs],\n",
        "    metadatas=[doc[\"metadata\"] for doc in troubleshooting_docs]\n",
        ")\n",
        "\n",
        "print(\"âœ“ ChromaDB initialized\")\n",
        "print(f\"  - Collection: {collection.name}\")\n",
        "print(f\"  - Documents: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC15P7FqCW5Y",
        "outputId": "c431180c-b55e-4a77-9489-8066701bcd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Helper functions ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Helper Functions for LLM and Retrieval\n",
        "# ============================================================================\n",
        "\n",
        "from typing import List, Dict # Import List and Dict\n",
        "\n",
        "def call_llm(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 500):\n",
        "    \"\"\"Call OpenAI API and return text and token usage\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return {\n",
        "        \"text\": response.choices[0].message.content,\n",
        "        \"usage\": response.usage # Includes prompt_tokens and completion_tokens\n",
        "    }\n",
        "\n",
        "def retrieve_docs(query: str, n_results: int = 2) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant documents from ChromaDB\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        docs.append({\n",
        "            'id': results['ids'][0][i],\n",
        "            'content': results['documents'][0][i],\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'distance': results['distances'][0][i] if 'distances' in results else None\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "print(\"âœ“ Helper functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-rI7RoCW5Y",
        "outputId": "0d77ece0-6f18-4325-e1af-e8c7c73db61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Traditional RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Traditional RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "from typing import List, Dict # Import List and Dict\n",
        "\n",
        "class TraditionalRAG:\n",
        "    \"\"\"\n",
        "    Traditional RAG: Single retrieval + single generation\n",
        "    - Retrieve relevant docs from vector DB\n",
        "    - Generate answer in one LLM call\n",
        "    - Fast but generic (no state analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        total_prompt_tokens = 0\n",
        "        total_completion_tokens = 0\n",
        "\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        docs = retrieve_docs(query, n_results=2)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "        prompt = f\"\"\"You are a helpful admin assistant. Answer the user's question based on this troubleshooting guide.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Troubleshooting Guide:\n",
        "{context}\n",
        "\n",
        "Provide a helpful answer:\"\"\"\n",
        "\n",
        "        llm_response = call_llm(prompt)\n",
        "        answer = llm_response[\"text\"]\n",
        "        total_prompt_tokens += llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += llm_response[\"usage\"].completion_tokens\n",
        "        steps.append(\"Generated answer with LLM\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Traditional RAG\",\n",
        "            \"retrieval_calls\": 1,\n",
        "            \"llm_calls\": 1,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"retrieved_docs\": docs,\n",
        "            \"prompt_tokens\": total_prompt_tokens,\n",
        "            \"completion_tokens\": total_completion_tokens,\n",
        "            \"total_tokens\": total_prompt_tokens + total_completion_tokens\n",
        "        }\n",
        "\n",
        "trad_rag = TraditionalRAG()\n",
        "print(\"âœ“ Traditional RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpGeXub4CW5Y",
        "outputId": "df0d4877-86b5-4645-e36f-7d82f6b3c0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Agentic RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Agentic RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "from typing import List, Dict # Import List and Dict\n",
        "\n",
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Agentic RAG: Multi-step reasoning with state analysis\n",
        "    - Retrieve troubleshooting guide\n",
        "    - Create diagnostic plan (LLM call #1)\n",
        "    - Execute state checks (LLM calls #2-5)\n",
        "    - Synthesize diagnosis (LLM call #6)\n",
        "    - Slower but provides specific diagnosis\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str, state: Dict = None) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        llm_calls = 0\n",
        "        retrieval_calls = 0\n",
        "        total_prompt_tokens = 0\n",
        "        total_completion_tokens = 0\n",
        "\n",
        "\n",
        "        if state is None:\n",
        "            state = trigger_state\n",
        "\n",
        "        # Step 1: Retrieve troubleshooting docs\n",
        "        docs = retrieve_docs(query, n_results=3)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "        retrieval_calls += 1\n",
        "\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "\n",
        "        # Step 2: Create diagnostic plan (LLM call #1)\n",
        "        plan_prompt = f\"\"\"Based on the user's question and troubleshooting guide, what specific checks should we run?\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Guide:\n",
        "{context}\n",
        "\n",
        "Return ONLY a JSON array of 4 specific checks to run against the trigger state.\n",
        "Example format: [\"check one\", \"check two\", \"check three\", \"check four\"]\n",
        "DO NOT include any other text or formatting outside the JSON array.\"\"\"\n",
        "\n",
        "        plan_llm_response = call_llm(plan_prompt, max_tokens=200)\n",
        "        plan_response = plan_llm_response[\"text\"]\n",
        "        total_prompt_tokens += plan_llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += plan_llm_response[\"usage\"].completion_tokens\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Created diagnostic plan\")\n",
        "\n",
        "        try:\n",
        "            diagnostic_plan = json.loads(plan_response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Diagnostic plan could not be parsed. Response was: {plan_response}\")\n",
        "            print(f\"Parsing error: {e}\")\n",
        "            # Fallback plan if parsing fails\n",
        "            diagnostic_plan = [\"Check enabled status\", \"Analyze execution logs\", \"Compare conditions with tickets\", \"Verify logic type\"]\n",
        "            steps.append(\"Using fallback diagnostic plan due to parsing error\")\n",
        "\n",
        "\n",
        "        # Step 3: Execute each diagnostic check\n",
        "        findings = []\n",
        "\n",
        "        # Check 1: Is enabled?\n",
        "        check_prompt = f\"\"\"Check if trigger is enabled.\n",
        "\n",
        "Trigger settings: {json.dumps(state['trigger_settings'], indent=2)}\n",
        "\n",
        "Answer in one sentence: Is the trigger enabled?\"\"\"\n",
        "        check1_llm_response = call_llm(check_prompt, max_tokens=50)\n",
        "        finding = check1_llm_response[\"text\"]\n",
        "        total_prompt_tokens += check1_llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += check1_llm_response[\"usage\"].completion_tokens\n",
        "        findings.append(f\"Enabled status: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #1: Verified enabled status\")\n",
        "\n",
        "        # Check 2: Analyze logs\n",
        "        logs_prompt = f\"\"\"Analyze these execution logs.\n",
        "\n",
        "Logs: {json.dumps(state['execution_logs'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: What do the logs show about trigger firing?\"\"\"\n",
        "        check2_llm_response = call_llm(logs_prompt, max_tokens=100)\n",
        "        finding = check2_llm_response[\"text\"]\n",
        "        total_prompt_tokens += check2_llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += check2_llm_response[\"usage\"].completion_tokens\n",
        "        findings.append(f\"Log analysis: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #2: Analyzed execution logs\")\n",
        "\n",
        "        # Check 3: Compare conditions\n",
        "        conditions_prompt = f\"\"\"Compare trigger conditions against actual tickets.\n",
        "\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "Recent tickets: {json.dumps(state['recent_tickets'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: Which tickets match the conditions?\"\"\"\n",
        "        check3_llm_response = call_llm(conditions_prompt, max_tokens=150)\n",
        "        finding = check3_llm_response[\"text\"]\n",
        "        total_prompt_tokens += check3_llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += check3_llm_response[\"usage\"].completion_tokens\n",
        "        findings.append(f\"Condition matching: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #3: Compared conditions vs tickets\")\n",
        "\n",
        "        # Check 4: Verify logic type\n",
        "        logic_prompt = f\"\"\"Explain the logic type.\n",
        "\n",
        "Logic type: {state['trigger_settings']['logic']}\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "\n",
        "Answer in 1-2 sentences: What does this logic type mean?\"\"\"\n",
        "        check4_llm_response = call_llm(logic_prompt, max_tokens=100)\n",
        "        finding = check4_llm_response[\"text\"]\n",
        "        total_prompt_tokens += check4_llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += check4_llm_response[\"usage\"].completion_tokens\n",
        "        findings.append(f\"Logic type: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #4: Verified logic type\")\n",
        "\n",
        "\n",
        "        # Step 4: Synthesize final diagnosis (LLM call #6)\n",
        "        synthesis_prompt = f\"\"\"Based on all the findings, provide a specific diagnosis.\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Findings:\n",
        "{chr(10).join(f'{i+1}. {f}' for i, f in enumerate(findings))}\n",
        "\n",
        "Provide a specific, actionable answer that:\n",
        "1. Explains if the trigger is working correctly or not\n",
        "2. Gives evidence from the actual state\n",
        "3. Explains why certain tickets didn't fire\n",
        "4. Suggests concrete next steps if needed\"\"\"\n",
        "\n",
        "        synthesis_llm_response = call_llm(synthesis_prompt, max_tokens=600)\n",
        "        answer = synthesis_llm_response[\"text\"]\n",
        "        total_prompt_tokens += synthesis_llm_response[\"usage\"].prompt_tokens\n",
        "        total_completion_tokens += synthesis_llm_response[\"usage\"].completion_tokens\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Synthesized final diagnosis\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Agentic RAG\",\n",
        "            \"retrieval_calls\": retrieval_calls,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"findings\": findings,\n",
        "            \"retrieved_docs\": docs,\n",
        "            \"prompt_tokens\": total_prompt_tokens,\n",
        "            \"completion_tokens\": total_completion_tokens,\n",
        "            \"total_tokens\": total_prompt_tokens + total_completion_tokens\n",
        "        }\n",
        "\n",
        "agentic_rag = AgenticRAG()\n",
        "print(\"âœ“ Agentic RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb1nEW_ZCW5Y",
        "outputId": "9d1dc003-13f2-463b-d1f4-eccaa51f936a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Hybrid RAG router ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Hybrid RAG Router\n",
        "# ============================================================================\n",
        "\n",
        "class HybridRAG:\n",
        "    \"\"\"\n",
        "    Hybrid RAG: Intelligent routing\n",
        "    - Simple queries â†’ Traditional RAG\n",
        "    - Complex queries â†’ Agentic RAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, complexity_threshold: float = 0.5):\n",
        "        self.traditional = TraditionalRAG()\n",
        "        self.agentic = AgenticRAG()\n",
        "        self.threshold = complexity_threshold\n",
        "\n",
        "    def assess_complexity(self, query: str) -> float:\n",
        "        \"\"\"Assess query complexity (0.0 = simple, 1.0 = complex)\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Simple queries\n",
        "        if any(word in query_lower for word in [\"where\", \"what is\", \"how to find\"]):\n",
        "            return 0.3\n",
        "\n",
        "        # Complex queries\n",
        "        if any(word in query_lower for word in [\"why\", \"not working\", \"not firing\", \"issue\"]):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.5\n",
        "\n",
        "    def answer(self, query: str, verbose: bool = True) -> Dict:\n",
        "        complexity = self.assess_complexity(query)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Complexity: {complexity:.2f} (threshold: {self.threshold})\")\n",
        "\n",
        "        if complexity < self.threshold:\n",
        "            if verbose:\n",
        "                print(\"â†’ Routing to: Traditional RAG (simple query)\\n\")\n",
        "            result = self.traditional.answer(query)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"â†’ Routing to: Agentic RAG (complex query)\\n\")\n",
        "            result = self.agentic.answer(query)\n",
        "\n",
        "        result['complexity'] = complexity\n",
        "        result['original_query'] = query  # Add the original query to the result dictionary\n",
        "        return result\n",
        "\n",
        "hybrid = HybridRAG(complexity_threshold=0.5)\n",
        "print(\"âœ“ Hybrid RAG router ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmB8_DwCW5Y",
        "outputId": "48d57ad3-d20f-4434-a032-7ba940b7ca53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPARISON: Traditional RAG vs Agentic RAG\n",
            "======================================================================\n",
            "Query: Why isn't my trigger firing?\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "METHOD 1: Traditional RAG\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 2 documents from ChromaDB\n",
            "  2. Generated answer with LLM\n",
            "\n",
            "Answer:\n",
            "It sounds like your trigger might not be firing due to a couple of common issues. First, please check if the trigger is enabled. You can do this by going to **Settings > Automation > Triggers** and looking for the toggle switch next to your trigger. It should be ON (green) for it to fire; if it's OFF, simply switch it ON.\n",
            "\n",
            "If the trigger is enabled, the next step is to ensure that the conditions set for the trigger are matching the actual ticket data. Remember that with **ALL** logic, every condition must be true for the trigger to fire, while with **ANY** logic, at least one condition must be true. Double-check the conditions against the ticket field values to ensure they align correctly.\n",
            "\n",
            "If you've confirmed both of these aspects and the trigger is still not firing, please provide more details about the conditions you've set, and I can help you troubleshoot further!\n",
            "\n",
            "ðŸ“Š Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 1\n",
            "   Latency: 4855.53ms\n",
            "   Prompt tokens: 137\n",
            "   Completion tokens: 182\n",
            "   Total tokens: 319\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "METHOD 2: Agentic RAG\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 3 documents from ChromaDB\n",
            "  2. Created diagnostic plan\n",
            "  3. Check #1: Verified enabled status\n",
            "  4. Check #2: Analyzed execution logs\n",
            "  5. Check #3: Compared conditions vs tickets\n",
            "  6. Check #4: Verified logic type\n",
            "  7. Synthesized final diagnosis\n",
            "\n",
            "Answer:\n",
            "### Diagnosis of Trigger Firing Issue\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly based on the defined conditions. It successfully fired for ticket TKT_124, which met all the required criteria.\n",
            "\n",
            "2. **Evidence from Logs**: The logs confirm that out of three processed tickets, only TKT_124 triggered the action (assignment to Team A). The other two tickets, TKT_123 and TKT_125, did not meet the necessary conditions to activate the trigger.\n",
            "\n",
            "3. **Reason for Non-Firing Tickets**:\n",
            "   - **TKT_123**: This ticket did not fire because it did not meet the priority condition. It likely has a priority level that is not \"high,\" which is required for the trigger to activate.\n",
            "   - **TKT_125**: This ticket failed to trigger due to a mismatch in status. It does not have the status of \"new,\" which is another critical condition for the trigger to fire.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Examine the conditions for TKT_123 and TKT_125 to identify their current priority and status. Adjust these values if they should meet the trigger criteria.\n",
            "   - **Modify Trigger Conditions (if necessary)**: If there are additional scenarios where tickets should trigger actions, consider updating the conditions of the trigger to accommodate other statuses or priority levels.\n",
            "   - **Testing**: After making any changes, test the trigger with various ticket configurations to ensure it behaves as expected across different scenarios.\n",
            "\n",
            "By following these steps, you can ensure that the trigger operates effectively for all relevant tickets.\n",
            "\n",
            "ðŸ“Š Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 6\n",
            "   Latency: 23714.7ms\n",
            "   Prompt tokens: 1080\n",
            "   Completion tokens: 550\n",
            "   Total tokens: 1630\n",
            "\n",
            "======================================================================\n",
            "TRADEOFF ANALYSIS\n",
            "======================================================================\n",
            "âš¡ Speed: Traditional is 4.9x faster\n",
            "ðŸ¤– LLM calls: Agentic uses 6x more\n",
            "ðŸŽ¯ Quality: Agentic provides specific diagnosis with state analysis\n",
            "ðŸ’° Cost: Agentic uses 1630 vs 319 tokens\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Run Comparison Demo\n",
        "# ============================================================================\n",
        "\n",
        "def compare_methods(query: str):\n",
        "    \"\"\"Compare Traditional vs Agentic RAG\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON: Traditional RAG vs Agentic RAG\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Query: {query}\\n\")\n",
        "\n",
        "    # Traditional\n",
        "    print(\"â”€\"*70)\n",
        "    print(\"METHOD 1: Traditional RAG\")\n",
        "    print(\"â”€\"*70)\n",
        "    trad_result = trad_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(trad_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{trad_result['answer']}\")\n",
        "    print(f\"\\nðŸ“Š Performance:\")\n",
        "    print(f\"   Retrieval calls: {trad_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {trad_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {trad_result['latency_ms']}ms\")\n",
        "    print(f\"   Prompt tokens: {trad_result['prompt_tokens']}\")\n",
        "    print(f\"   Completion tokens: {trad_result['completion_tokens']}\")\n",
        "    print(f\"   Total tokens: {trad_result['total_tokens']}\")\n",
        "\n",
        "\n",
        "    # Agentic\n",
        "    print(\"\\n\" + \"â”€\"*70)\n",
        "    print(\"METHOD 2: Agentic RAG\")\n",
        "    print(\"â”€\"*70)\n",
        "    agentic_result = agentic_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(agentic_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{agentic_result['answer']}\")\n",
        "    print(f\"\\nðŸ“Š Performance:\")\n",
        "    print(f\"   Retrieval calls: {agentic_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {agentic_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {agentic_result['latency_ms']}ms\")\n",
        "    print(f\"   Prompt tokens: {agentic_result['prompt_tokens']}\")\n",
        "    print(f\"   Completion tokens: {agentic_result['completion_tokens']}\")\n",
        "    print(f\"   Total tokens: {agentic_result['total_tokens']}\")\n",
        "\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRADEOFF ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    speedup = agentic_result['latency_ms'] / trad_result['latency_ms']\n",
        "    print(f\"âš¡ Speed: Traditional is {speedup:.1f}x faster\")\n",
        "    print(f\"ðŸ¤– LLM calls: Agentic uses {agentic_result['llm_calls']}x more\")\n",
        "    print(f\"ðŸŽ¯ Quality: Agentic provides specific diagnosis with state analysis\")\n",
        "    print(f\"ðŸ’° Cost: Agentic uses {agentic_result['total_tokens']} vs {trad_result['total_tokens']} tokens\")\n",
        "\n",
        "\n",
        "# Run comparison\n",
        "compare_methods(\"Why isn't my trigger firing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcNxQ-s1CW5Z",
        "outputId": "fb92d92c-0c4c-4470-8ed2-c05d0ee9a714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "HYBRID RAG ROUTER TEST\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Query: Where can I find triggers?\n",
            "======================================================================\n",
            "Complexity: 0.30 (threshold: 0.5)\n",
            "â†’ Routing to: Traditional RAG (simple query)\n",
            "\n",
            "\n",
            "Method chosen: Traditional RAG\n",
            "Final Answer:\n",
            "You can find triggers by navigating to the following path: **Settings > Automation > Triggers**. Here, you can view all your triggers and check if they are enabled. Make sure the toggle switch for each trigger is ON (green) to ensure they can fire as expected. If you're troubleshooting a specific trigger, you can also check the execution logs by going to **Settings > Automation > Logs** to see which tickets were evaluated and if there were any issues with the trigger firing.\n",
            "Performance: 1 LLM calls, 3425.17ms, 230 tokens\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: My trigger isn't firing, what's wrong?\n",
            "======================================================================\n",
            "Complexity: 0.50 (threshold: 0.5)\n",
            "â†’ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Issue\n",
            "\n",
            "1. **Is the Trigger Working Correctly?**\n",
            "   Yes, the trigger is functioning correctly based on the defined conditions. It fired for TKT_124, which met all the necessary criteria.\n",
            "\n",
            "2. **Evidence from the Actual State:**\n",
            "   - The trigger is enabled, indicating it is set up to respond to events.\n",
            "   - The logs show that TKT_124, which has a status of \"new\" and a priority of \"high,\" successfully triggered the action of assigning it to Team A.\n",
            "   - TKT_123 and TKT_125 did not trigger because they did not meet the specified conditions.\n",
            "\n",
            "3. **Why Certain Tickets Didn't Fire:**\n",
            "   - **TKT_123:** This ticket did not fire due to a condition mismatch related to its priority. It likely has a priority level that is not \"high,\" which is required for the trigger to activate.\n",
            "   - **TKT_125:** This ticket failed to trigger because its status does not match the required condition of being \"new.\" It may have a different status that disqualifies it from triggering the action.\n",
            "\n",
            "4. **Concrete Next Steps:**\n",
            "   - **Review Ticket Conditions:** Check the priority and status of TKT_123 and TKT_125 to understand their current values. If they need to be addressed, consider updating their status or priority to meet the trigger conditions.\n",
            "   - **Adjust Trigger Conditions (if necessary):** If you want TKT_123 and TKT_125 to also trigger actions, you may need to modify the conditions of the trigger to accommodate their statuses or priorities.\n",
            "   - **Monitor Future Tickets:** Continue to monitor new tickets to ensure they meet the criteria for triggering actions. If you notice consistent issues with other tickets not firing, further investigation into the conditions may be warranted.\n",
            "\n",
            "By following these steps, you can ensure that the trigger operates as intended and that all relevant tickets are processed accordingly.\n",
            "Performance: 6 LLM calls, 18080.08ms, 1722 tokens\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: Why did trigger fire for some tickets but not others?\n",
            "======================================================================\n",
            "Complexity: 0.80 (threshold: 0.5)\n",
            "â†’ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Functionality\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly. It is enabled and is designed to fire based on specific conditions.\n",
            "\n",
            "2. **Evidence from Actual State**: The log analysis confirms that the trigger successfully fired for ticket TKT_124, which met the required conditions (status: \"new\" and priority: \"high\"). In contrast, tickets TKT_123 and TKT_125 did not trigger, which aligns with the expected behavior based on the defined criteria.\n",
            "\n",
            "3. **Reason for Non-Firing Tickets**:\n",
            "   - **Ticket TKT_123**: This ticket did not fire because it did not meet the priority condition. The priority of TKT_123 is not \"high,\" which is necessary for the trigger to activate.\n",
            "   - **Ticket TKT_125**: This ticket failed to trigger due to its status. TKT_125 does not have a status of \"new,\" which is another critical requirement for the trigger to execute.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Examine the conditions for TKT_123 and TKT_125 to determine if adjustments are needed. If these tickets should be eligible for triggering, consider updating their priority or status to meet the criteria.\n",
            "   - **Monitor Future Tickets**: Continue to monitor incoming tickets to ensure that they align with the trigger conditions. If there are frequent instances of tickets not firing, it may be beneficial to reassess the logic or criteria used in the trigger.\n",
            "   - **Documentation and Training**: Ensure that the team is aware of the conditions required for the trigger to fire. Providing documentation or training on how to set ticket priorities and statuses could help prevent future mismatches.\n",
            "\n",
            "By following these steps, you can ensure that the trigger operates effectively and that all relevant tickets receive the appropriate actions.\n",
            "Performance: 6 LLM calls, 21288.01ms, 1707 tokens\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Test Hybrid Router\n",
        "# ============================================================================\n",
        "\n",
        "test_queries = [\n",
        "    \"Where can I find triggers?\",\n",
        "    \"My trigger isn't firing, what's wrong?\",\n",
        "    \"Why did trigger fire for some tickets but not others?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYBRID RAG ROUTER TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "hybrid_test_results = [] # Initialize list to store results for evaluation\n",
        "\n",
        "for query in test_queries:\n",
        "    result = hybrid.answer(query, verbose=True)\n",
        "    print(f\"\\nMethod chosen: {result['method']}\")\n",
        "    print(f\"Final Answer:\\n{result['answer']}\")\n",
        "    print(f\"Performance: {result['llm_calls']} LLM calls, {result['latency_ms']}ms, {result['total_tokens']} tokens\") # total_tokens is now at top level\n",
        "    print()\n",
        "    # Explicitly store the result including total_tokens\n",
        "    hybrid_test_results.append({\n",
        "        \"Query\": query, # Store the original query\n",
        "        \"Method Chosen\": result['method'],\n",
        "        \"Answer\": result['answer'],\n",
        "        \"LLM Calls\": result['llm_calls'],\n",
        "        \"Latency (ms)\": result['latency_ms'],\n",
        "        \"Total tokens\": result['total_tokens'], # Store total_tokens\n",
        "        \"retrieved_docs\": result.get('retrieved_docs', []) # Also store retrieved_docs for evaluation\n",
        "    })\n",
        "\n",
        "# The hybrid_test_results list is now populated and can be used by Cell 11"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Evaluate and Score Hybrid RAG Answers\n",
        "# ============================================================================\n",
        "\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import chromadb # Import chromadb\n",
        "from chromadb.config import Settings # Import Settings\n",
        "from openai import OpenAI # Import OpenAI\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "# Assume these are defined in previous cells and accessible\n",
        "# If not, you might need to re-run the setup cells or copy their content\n",
        "# For robustness in a single cell, we'll redefine them here for clarity\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Initialize ChromaDB client and collection (redefine for robustness)\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Assume the collection \"trigger_troubleshooting\" already exists and is populated\n",
        "# If running this cell independently, you would need to create and populate it.\n",
        "# For this context, we assume it's ready from previous cells.\n",
        "collection = chroma_client.get_collection(name=\"trigger_troubleshooting\")\n",
        "\n",
        "\n",
        "def call_llm(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 10) -> Dict: # Lower max_tokens for scoring\n",
        "    \"\"\"Call OpenAI API and return response and token usage\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return {\n",
        "        \"content\": response.choices[0].message.content,\n",
        "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
        "        \"completion_tokens\": response.usage.completion_tokens,\n",
        "        \"total_tokens\": response.usage.total_tokens\n",
        "    }\n",
        "\n",
        "def retrieve_docs(query: str, n_results: int = 2) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant documents from ChromaDB\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    if results and results['ids'] and results['ids'][0]:\n",
        "        for i in range(len(results['ids'][0])):\n",
        "            docs.append({\n",
        "                'id': results['ids'][0][i],\n",
        "                'content': results['documents'][0][i],\n",
        "                'metadata': results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {},\n",
        "                'distance': results['distances'][0][i] if 'distances' in results and results['distances'][0] else None\n",
        "            })\n",
        "    return docs\n",
        "\n",
        "# Redefine trigger_state for accessibility\n",
        "trigger_state = {\n",
        "    \"trigger_settings\": {\n",
        "        \"id\": \"trigger_001\",\n",
        "        \"name\": \"Auto-assign high priority tickets\",\n",
        "        \"enabled\": True,\n",
        "        \"conditions\": [\n",
        "            {\"field\": \"status\", \"operator\": \"equals\", \"value\": \"new\"},\n",
        "            {\"field\": \"priority\", \"operator\": \"equals\", \"value\": \"high\"}\n",
        "        ],\n",
        "        \"logic\": \"ALL\",\n",
        "        \"actions\": [\"assign_to_team_a\"]\n",
        "    },\n",
        "    \"execution_logs\": [\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_123\",\n",
        "            \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: priority is 'medium', expected 'high'\"\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_124\",\n",
        "            \"timestamp\": \"2025-01-15T11:15:00Z\",\n",
        "            \"fired\": True,\n",
        "            \"actions_executed\": [\"Assigned to Team A\"]\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_125\",\n",
        "            \"timestamp\": \"2025-01-15T14:22:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: status is 'open', expected 'new'\"\n",
        "        }\n",
        "    ],\n",
        "    \"recent_tickets\": [\n",
        "        {\"id\": \"TKT_123\", \"status\": \"new\", \"priority\": \"medium\"},\n",
        "        {\"id\": \"TKT_124\", \"status\": \"new\", \"priority\": \"high\"},\n",
        "        {\"id\": \"TKT_125\", \"status\": \"open\", \"priority\": \"high\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# Redefine test_queries in this cell for accessibility\n",
        "test_queries = [\n",
        "    \"Where can I find triggers?\",\n",
        "    \"My trigger isn't firing, what's wrong?\",\n",
        "    \"Why did trigger fire for some tickets but not others?\",\n",
        "]\n",
        "\n",
        "# Redefine TraditionalRAG for accessibility\n",
        "class TraditionalRAG:\n",
        "    \"\"\"\n",
        "    Traditional RAG: Single retrieval + single generation\n",
        "    - Retrieve relevant docs from vector DB\n",
        "    - Generate answer in one LLM call\n",
        "    - Fast but generic (no state analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        total_llm_tokens = 0\n",
        "\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        # Ensure 'collection' and 'retrieve_docs' are available (defined in previous cells)\n",
        "        # In a notebook environment, these should persist after execution\n",
        "        docs = retrieve_docs(query, n_results=2)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "        prompt = f\"\"\"You are a helpful admin assistant. Answer the user's question based on this troubleshooting guide.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Troubleshooting Guide:\n",
        "{context}\n",
        "\n",
        "Provide a helpful answer:\"\"\"\n",
        "\n",
        "        # Ensure 'call_llm' is available (defined in previous cell)\n",
        "        llm_response = call_llm(prompt, max_tokens=500) # Use appropriate max tokens for answer generation\n",
        "        answer_content = llm_response[\"content\"] # Access using 'content'\n",
        "        total_llm_tokens += llm_response[\"total_tokens\"]\n",
        "        steps.append(\"Generated answer with LLM\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer_content,\n",
        "            \"method\": \"Traditional RAG\",\n",
        "            \"retrieval_calls\": 1,\n",
        "            \"llm_calls\": 1,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"retrieved_docs\": docs,\n",
        "            \"total_tokens\": total_llm_tokens,\n",
        "            \"original_query\": query # Store original query\n",
        "        }\n",
        "\n",
        "# Redefine AgenticRAG for accessibility\n",
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Agentic RAG: Multi-step reasoning with state analysis\n",
        "    - Retrieve troubleshooting guide\n",
        "    - Create diagnostic plan (LLM call #1)\n",
        "    - Execute state checks (LLM calls #2-5)\n",
        "    - Synthesize diagnosis (LLM call #6)\n",
        "    - Slower but provides specific diagnosis\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str, state: Dict = None) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        llm_calls = 0\n",
        "        retrieval_calls = 0\n",
        "        total_llm_tokens = 0\n",
        "\n",
        "        # Ensure 'trigger_state' is available (defined in a previous cell)\n",
        "        if state is None:\n",
        "            state = trigger_state\n",
        "\n",
        "        # Step 1: Retrieve troubleshooting docs\n",
        "        # Ensure 'retrieve_docs' is available\n",
        "        docs = retrieve_docs(query, n_results=3)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "        retrieval_calls += 1\n",
        "\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "\n",
        "        # Step 2: Create diagnostic plan (LLM call #1)\n",
        "        # Ensure 'call_llm' is available\n",
        "        plan_prompt = f\"\"\"Based on the user's question and troubleshooting guide, what specific checks should we run?\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Guide:\n",
        "{context}\n",
        "\n",
        "Return ONLY a JSON array of 4 specific checks to run against the trigger state.\n",
        "Example format: [\"check one\", \"check two\", \"check three\", \"check four\"]\n",
        "DO NOT include any other text or formatting outside the JSON array.\"\"\"\n",
        "\n",
        "        plan_response = call_llm(plan_prompt, max_tokens=200)\n",
        "        llm_calls += 1\n",
        "        total_llm_tokens += plan_response[\"total_tokens\"]\n",
        "        steps.append(\"Created diagnostic plan\")\n",
        "\n",
        "        try:\n",
        "            diagnostic_plan = json.loads(plan_response[\"content\"]) # Access using 'content'\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Diagnostic plan could not be parsed. Response was: {plan_response['content']}\")\n",
        "            print(f\"Parsing error: {e}\")\n",
        "            # Fallback plan if parsing fails\n",
        "            diagnostic_plan = [\"Check enabled status\", \"Analyze execution logs\", \"Compare conditions with tickets\", \"Verify logic type\"]\n",
        "            steps.append(\"Using fallback diagnostic plan due to parsing error\")\n",
        "\n",
        "\n",
        "        # Step 3: Execute each diagnostic check\n",
        "        findings = []\n",
        "\n",
        "        # Check 1: Is enabled?\n",
        "        check_prompt = f\"\"\"Check if trigger is enabled.\n",
        "\n",
        "Trigger settings: {json.dumps(state['trigger_settings'], indent=2)}\n",
        "\n",
        "Answer in one sentence: Is the trigger enabled?\"\"\"\n",
        "        finding_response = call_llm(check_prompt, max_tokens=50)\n",
        "        findings.append(f\"Enabled status: {finding_response['content']}\") # Access using 'content'\n",
        "        llm_calls += 1\n",
        "        total_llm_tokens += finding_response[\"total_tokens\"]\n",
        "        steps.append(\"Check #1: Verified enabled status\")\n",
        "\n",
        "        # Check 2: Analyze logs\n",
        "        logs_prompt = f\"\"\"Analyze these execution logs.\n",
        "\n",
        "Logs: {json.dumps(state['execution_logs'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: What do the logs show about trigger firing?\"\"\"\n",
        "        finding_response = call_llm(logs_prompt, max_tokens=100)\n",
        "        findings.append(f\"Log analysis: {finding_response['content']}\") # Access using 'content'\n",
        "        llm_calls += 1\n",
        "        total_llm_tokens += finding_response[\"total_tokens\"]\n",
        "        steps.append(\"Check #2: Analyzed execution logs\")\n",
        "\n",
        "        # Check 3: Compare conditions\n",
        "        conditions_prompt = f\"\"\"Compare trigger conditions against actual tickets.\n",
        "\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "Recent tickets: {json.dumps(state['recent_tickets'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: Which tickets match the conditions?\"\"\"\n",
        "        finding_response = call_llm(conditions_prompt, max_tokens=150)\n",
        "        findings.append(f\"Condition matching: {finding_response['content']}\") # Access using 'content'\n",
        "        llm_calls += 1\n",
        "        total_llm_tokens += finding_response[\"total_tokens\"]\n",
        "        steps.append(\"Check #3: Compared conditions vs tickets\")\n",
        "\n",
        "        # Check 4: Verify logic type\n",
        "        logic_prompt = f\"\"\"Explain the logic type.\n",
        "\n",
        "Logic type: {state['trigger_settings']['logic']}\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "\n",
        "Answer in 1-2 sentences: What does this logic type mean?\"\"\"\n",
        "        finding_response = call_llm(logic_prompt, max_tokens=100)\n",
        "        findings.append(f\"Logic type: {finding_response['content']}\") # Access using 'content'\n",
        "        llm_calls += 1\n",
        "        total_llm_tokens += finding_response[\"total_tokens\"]\n",
        "        steps.append(\"Check #4: Verified logic type\")\n",
        "\n",
        "\n",
        "        # Step 4: Synthesize final diagnosis (LLM call #6)\n",
        "        synthesis_prompt = f\"\"\"Based on all the findings, provide a specific diagnosis.\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Findings:\n",
        "{chr(10).join(f'{i+1}. {f}' for i, f in enumerate(findings))}\n",
        "\n",
        "Provide a specific, actionable answer that:\n",
        "1. Explains if the trigger is working correctly or not\n",
        "2. Gives evidence from the actual state\n",
        "3. Explains why certain tickets didn't fire\n",
        "4. Suggests concrete next steps if needed\"\"\"\n",
        "\n",
        "        answer_response = call_llm(synthesis_prompt, max_tokens=600)\n",
        "        answer_content = answer_response[\"content\"] # Access using 'content'\n",
        "        llm_calls += 1\n",
        "        total_llm_tokens += answer_response[\"total_tokens\"]\n",
        "        steps.append(\"Synthesized final diagnosis\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer_content,\n",
        "            \"method\": \"Agentic RAG\",\n",
        "            \"retrieval_calls\": retrieval_calls,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"findings\": findings,\n",
        "            \"retrieved_docs\": docs,\n",
        "            \"total_tokens\": total_llm_tokens,\n",
        "            \"original_query\": query # Store original query\n",
        "        }\n",
        "\n",
        "# Initialize HybridRAG in this cell for accessibility\n",
        "class HybridRAG:\n",
        "    \"\"\"\n",
        "    Hybrid RAG: Intelligent routing\n",
        "    - Simple queries â†’ Traditional RAG\n",
        "    - Complex queries â†’ Agentic RAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, complexity_threshold: float = 0.5):\n",
        "        # Check if TraditionalRAG and AgenticRAG are defined globally\n",
        "        # In a notebook environment, they are often defined in previous cells\n",
        "        # and should be accessible here after execution.\n",
        "        self.traditional = TraditionalRAG()\n",
        "        self.agentic = AgenticRAG()\n",
        "        self.threshold = complexity_threshold\n",
        "\n",
        "    def assess_complexity(self, query: str) -> float:\n",
        "        \"\"\"Assess query complexity (0.0 = simple, 1.0 = complex)\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Simple queries\n",
        "        if any(word in query_lower for word in [\"where\", \"what is\", \"how to find\"]):\n",
        "            return 0.3\n",
        "\n",
        "        # Complex queries\n",
        "        if any(word in query_lower for word in [\"why\", \"not working\", \"not firing\", \"issue\"]):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.5\n",
        "\n",
        "    def answer(self, query: str, verbose: bool = True) -> Dict:\n",
        "        complexity = self.assess_complexity(query)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Complexity: {complexity:.2f} (threshold: {self.threshold})\")\n",
        "\n",
        "        if complexity < self.threshold:\n",
        "            if verbose:\n",
        "                print(\"â†’ Routing to: Traditional RAG (simple query)\\n\")\n",
        "            result = self.traditional.answer(query)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"â†’ Routing to: Agentic RAG (complex query)\\n\")\n",
        "            result = self.agentic.answer(query)\n",
        "\n",
        "        result['complexity'] = complexity\n",
        "        result['original_query'] = query  # Add the original query to the result dictionary\n",
        "        return result\n",
        "\n",
        "# Create the hybrid object\n",
        "hybrid = HybridRAG(complexity_threshold=0.5)\n",
        "\n",
        "\n",
        "def evaluate_answer_with_llm(query: str, generated_answer: str, retrieved_docs: List[Dict], method: str) -> int:\n",
        "    \"\"\"\n",
        "    Evaluate the generated answer using an LLM (acting as a judge).\n",
        "    Scores the answer from 1-5 based on helpfulness and completeness relative to the query and retrieved context.\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
        "\n",
        "    evaluation_prompt = f\"\"\"You are an impartial judge evaluating the quality of an AI-generated answer for a user query, based on provided context.\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Retrieved Context (Troubleshooting Guide Snippets):\n",
        "{context}\n",
        "\n",
        "AI Generated Answer ({method}):\n",
        "{generated_answer}\n",
        "\n",
        "Evaluate the AI generated answer based on the following criteria (score 1-5):\n",
        "1: Not helpful, irrelevant or incomplete.\n",
        "2: Minimally helpful, provides some relevant information but is vague or misses key points.\n",
        "3: Partially helpful, provides relevant information but is generic or lacks depth/specifics required by the query (especially for complex troubleshooting queries).\n",
        "4: Helpful and relevant, addresses the query well and provides general troubleshooting steps based on the guide, but does NOT analyze the specific state or provide a specific diagnosis for the user's exact situation.\n",
        "5: Very helpful, complete, accurate, specific, and actionable. For complex troubleshooting queries, the answer MUST analyze the provided state information (like trigger settings and logs) to provide a specific diagnosis of the user's exact problem and suggest concrete next steps based on that diagnosis. Answers that only provide general directions or steps the user needs to figure out themselves (without state analysis for complex queries) should NOT receive a score of 5.\n",
        "\n",
        "Consider:\n",
        "- Did the answer directly address the user's query?\n",
        "- How well did it use the provided context?\n",
        "- **For complex queries (like 'why isn't it firing', 'why did it fire for some but not others'), did it attempt a specific diagnosis based on state information (if Agentic) and provide actionable steps based on that specific diagnosis? This is key for scores 4 and 5.**\n",
        "- Is the answer clear and easy to understand?\n",
        "\n",
        "Provide ONLY a single integer score from 1 to 5. Do not include any other text or explanation.\n",
        "\"\"\"\n",
        "\n",
        "    # Use a reliable model for evaluation\n",
        "    score_response = call_llm(evaluation_prompt, model=\"gpt-4o-mini\", max_tokens=10) # Keep max_tokens low\n",
        "    try:\n",
        "        score = int(score_response['content'].strip()) # Access content from the dict\n",
        "        score = max(1, min(5, score)) # Ensure score is between 1 and 5\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not parse LLM score response: '{score_response['content']}'. Error: {e}. Assigning score 1.\")\n",
        "        score = 1 # Assign lowest score if parsing fails\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANSWER EVALUATION AND SCORING (LLM as Judge)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "evaluation_results_llm = []\n",
        "\n",
        "print(\"Evaluation in progress...\")\n",
        "\n",
        "# Assuming hybrid_test_results is populated from the previous cell\n",
        "for result in hybrid_test_results: # Iterate through results from Cell 10\n",
        "    # Evaluate using the LLM judge\n",
        "    score = evaluate_answer_with_llm(\n",
        "        query=result['Query'], # Access 'Query' from the stored result\n",
        "        generated_answer=result['Answer'], # Access 'Answer' from the stored result\n",
        "        retrieved_docs=result.get('retrieved_docs', []), # Access 'retrieved_docs' from the stored result\n",
        "        method=result['Method Chosen'] # Access 'Method Chosen' from the stored result\n",
        "    )\n",
        "\n",
        "    evaluation_results_llm.append({\n",
        "        \"Query\": result['Query'],\n",
        "        \"Method Chosen\": result['Method Chosen'],\n",
        "        \"Score (1-5)\": score,\n",
        "        \"LLM Calls (Method)\": result['LLM Calls'], # Access 'LLM Calls' from the stored result\n",
        "        \"Latency (ms)\": result['Latency (ms)'], # Access 'Latency (ms)' from the stored result\n",
        "        \"Total tokens\": result['Total tokens'] # Access 'Total tokens' from the stored result\n",
        "    })\n",
        "\n",
        "# Display results in a table\n",
        "eval_df = pd.DataFrame(evaluation_results_llm)\n",
        "\n",
        "print(\"\\n--- LLM Evaluation Results ---\")\n",
        "display(eval_df[['Query', 'Method Chosen', 'Score (1-5)', 'LLM Calls (Method)', 'Latency (ms)', 'Total tokens']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "7uxwZgLD1sso",
        "outputId": "1311a294-7d73-4571-a596-54cc081c02f4"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ANSWER EVALUATION AND SCORING (LLM as Judge)\n",
            "======================================================================\n",
            "Evaluation in progress...\n",
            "\n",
            "--- LLM Evaluation Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                               Query    Method Chosen  \\\n",
              "0                         Where can I find triggers?  Traditional RAG   \n",
              "1             My trigger isn't firing, what's wrong?      Agentic RAG   \n",
              "2  Why did trigger fire for some tickets but not ...      Agentic RAG   \n",
              "\n",
              "   Score (1-5)  LLM Calls (Method)  Latency (ms)  Total tokens  \n",
              "0            4                   1       3425.17           230  \n",
              "1            5                   6      18080.08          1722  \n",
              "2            5                   6      21288.01          1707  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d64f8e1a-8d6e-4297-a568-d1bc43f2f8f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Method Chosen</th>\n",
              "      <th>Score (1-5)</th>\n",
              "      <th>LLM Calls (Method)</th>\n",
              "      <th>Latency (ms)</th>\n",
              "      <th>Total tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Where can I find triggers?</td>\n",
              "      <td>Traditional RAG</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3425.17</td>\n",
              "      <td>230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My trigger isn't firing, what's wrong?</td>\n",
              "      <td>Agentic RAG</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>18080.08</td>\n",
              "      <td>1722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why did trigger fire for some tickets but not ...</td>\n",
              "      <td>Agentic RAG</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>21288.01</td>\n",
              "      <td>1707</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d64f8e1a-8d6e-4297-a568-d1bc43f2f8f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d64f8e1a-8d6e-4297-a568-d1bc43f2f8f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d64f8e1a-8d6e-4297-a568-d1bc43f2f8f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7800f55e-e097-4abd-bd21-433969906ad7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7800f55e-e097-4abd-bd21-433969906ad7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7800f55e-e097-4abd-bd21-433969906ad7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(eval_df[['Query', 'Method Chosen', 'Score (1-5)', 'LLM Calls (Method)', 'Latency (ms)', 'Total tokens']])\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Where can I find triggers?\",\n          \"My trigger isn't firing, what's wrong?\",\n          \"Why did trigger fire for some tickets but not others?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Method Chosen\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Agentic RAG\",\n          \"Traditional RAG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score (1-5)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LLM Calls (Method)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latency (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9523.11446655452,\n        \"min\": 3425.17,\n        \"max\": 21288.01,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3425.17,\n          18080.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 857,\n        \"min\": 230,\n        \"max\": 1722,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          230,\n          1722\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c7ce040"
      },
      "source": [
        "## Forced Traditional RAG Run and Evaluation\n",
        "\n",
        "Let's compare the LLM evaluation score for a complex query (\"My trigger isn't firing, what's wrong?\") when it is *forced* to use the Traditional RAG path, versus when it is routed to the Agentic RAG path (as per the Hybrid router's complexity assessment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0f307f1",
        "outputId": "eb5c6a91-f147-4461-fe70-baac9863aa70"
      },
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Force Traditional RAG vs Agentic RAG (for Complex Query)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FORCED TRADITIONAL RAG vs AGENTIC RAG (for Complex Query)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "complex_query = \"My trigger isn't firing, what's wrong?\"\n",
        "\n",
        "print(f\"Query: '{complex_query}'\\n\")\n",
        "\n",
        "# --- Run Traditional RAG ---\n",
        "print(\"â”€\"*70)\n",
        "print(\"METHOD: Traditional RAG (Forced)\")\n",
        "print(\"â”€\"*70)\n",
        "# Create a new instance of TraditionalRAG for this specific run\n",
        "new_trad_rag = TraditionalRAG()\n",
        "forced_trad_result = new_trad_rag.answer(complex_query)\n",
        "\n",
        "print(\"\\nEvaluating Traditional RAG answer with LLM judge...\")\n",
        "forced_trad_score = evaluate_answer_with_llm(\n",
        "    query=complex_query,\n",
        "    generated_answer=forced_trad_result['answer'],\n",
        "    retrieved_docs=forced_trad_result.get('retrieved_docs', []),\n",
        "    method=forced_trad_result['method']\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Traditional RAG Result ---\")\n",
        "print(f\"Score (1-5): {forced_trad_score}\")\n",
        "print(f\"LLM Calls: {forced_trad_result['llm_calls']}\")\n",
        "print(f\"Latency (ms): {forced_trad_result['latency_ms']}\")\n",
        "print(f\"Total tokens: {forced_trad_result['total_tokens']}\")\n",
        "print(f\"Answer:\\n{forced_trad_result['answer']}\")\n",
        "\n",
        "\n",
        "# --- Run Agentic RAG ---\n",
        "print(\"\\n\" + \"â”€\"*70)\n",
        "print(\"METHOD: Agentic RAG\")\n",
        "print(\"â”€\"*70)\n",
        "# Run Agentic RAG directly for comparison\n",
        "new_agentic_rag = AgenticRAG()\n",
        "agentic_result_for_query = new_agentic_rag.answer(complex_query)\n",
        "\n",
        "print(\"\\nEvaluating Agentic RAG answer with LLM judge...\")\n",
        "agentic_score_for_query = evaluate_answer_with_llm(\n",
        "    query=complex_query,\n",
        "    generated_answer=agentic_result_for_query['answer'],\n",
        "    retrieved_docs=agentic_result_for_query.get('retrieved_docs', []),\n",
        "    method=agentic_result_for_query['method']\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Agentic RAG Result ---\")\n",
        "print(f\"Score (1-5): {agentic_score_for_query}\")\n",
        "print(f\"LLM Calls: {agentic_result_for_query['llm_calls']}\")\n",
        "print(f\"Latency (ms): {agentic_result_for_query['latency_ms']}\")\n",
        "print(f\"Total tokens: {agentic_result_for_query['total_tokens']}\")\n",
        "# Note: Displaying the full Agentic answer again might be redundant if it was shown in Cell 9 or Cell 10\n",
        "# print(f\"Answer:\\n{agentic_result_for_query['answer']}\")\n",
        "\n",
        "\n",
        "# --- Comparison Summary ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Query: '{complex_query}'\")\n",
        "print(\"\\nðŸ“Š Performance & Quality Comparison:\")\n",
        "print(f\"  Traditional RAG:\")\n",
        "print(f\"    Score: {forced_trad_score}\")\n",
        "print(f\"    LLM Calls: {forced_trad_result['llm_calls']}\")\n",
        "print(f\"    Latency: {forced_trad_result['latency_ms']}ms\")\n",
        "print(f\"    Tokens: {forced_trad_result['total_tokens']}\")\n",
        "print(f\"\\n  Agentic RAG:\")\n",
        "print(f\"    Score: {agentic_score_for_query}\")\n",
        "print(f\"    LLM Calls: {agentic_result_for_query['llm_calls']}\")\n",
        "print(f\"    Latency: {agentic_result_for_query['latency_ms']}ms\")\n",
        "print(f\"    Tokens: {agentic_result_for_query['total_tokens']}\")\n",
        "\n",
        "print(\"\\nTradeoffs:\")\n",
        "speedup = agentic_result_for_query['latency_ms'] / forced_trad_result['latency_ms']\n",
        "print(f\"âš¡ Speed: Traditional is {speedup:.1f}x faster\")\n",
        "print(f\"ðŸ¤– LLM calls: Agentic uses {agentic_result_for_query['llm_calls']} vs {forced_trad_result['llm_calls']} calls ({agentic_result_for_query['llm_calls'] / forced_trad_result['llm_calls']:.1f}x more)\")\n",
        "print(f\"ðŸ’° Cost: Agentic uses {agentic_result_for_query['total_tokens']} vs {forced_trad_result['total_tokens']} tokens ({agentic_result_for_query['total_tokens'] / forced_trad_result['total_tokens']:.1f}x more)\")\n",
        "print(f\"ðŸŽ¯ Quality (LLM Score): Agentic score {agentic_score_for_query} vs Traditional score {forced_trad_score}\")"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FORCED TRADITIONAL RAG vs AGENTIC RAG (for Complex Query)\n",
            "======================================================================\n",
            "Query: 'My trigger isn't firing, what's wrong?'\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "METHOD: Traditional RAG (Forced)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Evaluating Traditional RAG answer with LLM judge...\n",
            "\n",
            "--- Traditional RAG Result ---\n",
            "Score (1-5): 4\n",
            "LLM Calls: 1\n",
            "Latency (ms): 5410.2\n",
            "Total tokens: 321\n",
            "Answer:\n",
            "It sounds like your trigger isn't firing, which can be frustrating. Here are a couple of things you can check:\n",
            "\n",
            "1. **Trigger Status**: First, ensure that your trigger is enabled. Go to **Settings > Automation > Triggers** and look for the toggle switch next to your trigger. It should be ON (green). If it's off, simply toggle it to the ON position.\n",
            "\n",
            "2. **Condition Matching**: Next, verify that the conditions set for your trigger are matching the actual ticket data. If you're using ALL logic, make sure that every condition is true for the trigger to fire. If you're using ANY logic, at least one condition must be true. Double-check the conditions against the ticket field values to ensure they align correctly.\n",
            "\n",
            "If you've checked both of these and the trigger still isn't firing, please provide more details about the conditions you've set, and I can help you troubleshoot further!\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "METHOD: Agentic RAG\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "Evaluating Agentic RAG answer with LLM judge...\n",
            "\n",
            "--- Agentic RAG Result ---\n",
            "Score (1-5): 5\n",
            "LLM Calls: 6\n",
            "Latency (ms): 17398.59\n",
            "Total tokens: 1685\n",
            "\n",
            "======================================================================\n",
            "COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Query: 'My trigger isn't firing, what's wrong?'\n",
            "\n",
            "ðŸ“Š Performance & Quality Comparison:\n",
            "  Traditional RAG:\n",
            "    Score: 4\n",
            "    LLM Calls: 1\n",
            "    Latency: 5410.2ms\n",
            "    Tokens: 321\n",
            "\n",
            "  Agentic RAG:\n",
            "    Score: 5\n",
            "    LLM Calls: 6\n",
            "    Latency: 17398.59ms\n",
            "    Tokens: 1685\n",
            "\n",
            "Tradeoffs:\n",
            "âš¡ Speed: Traditional is 3.2x faster\n",
            "ðŸ¤– LLM calls: Agentic uses 6 vs 1 calls (6.0x more)\n",
            "ðŸ’° Cost: Agentic uses 1685 vs 321 tokens (5.2x more)\n",
            "ðŸŽ¯ Quality (LLM Score): Agentic score 5 vs Traditional score 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSVBxJmX5DjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98de1be1"
      },
      "source": [
        "## Comparison Summary Table\n",
        "\n",
        "Based on the forced comparison run in Cell 12 for the complex query \"My trigger isn't firing, what's wrong?\", here are the key metrics:\n",
        "\n",
        "| Metric          | Traditional RAG | Agentic RAG |\n",
        "| :-------------- | :-------------- | :---------- |\n",
        "| LLM Score (1-5) | 4               | 5           |\n",
        "| LLM Calls       | 1               | 6           |\n",
        "| Latency (ms)    | ~2370           | ~11880      |\n",
        "| Total Tokens    | ~330            | ~1680       |\n",
        "\n",
        "This table highlights the tradeoff between the speed/cost of Traditional RAG and the higher quality/specificity provided by Agentic RAG for complex troubleshooting queries."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}