{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrikantvarma/AgenticAI/blob/main/Adaptive_RAG_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c22768"
      },
      "source": [
        "# Adaptive/Hybrid RAG: Traditional +  Agentic RAG\n",
        "\n",
        "**A Practical Comparison with ChromaDB and OpenAI**\n",
        "\n",
        "This notebook explores **Adaptive/Hybrid Retrieval-Augmented Generation (RAG)**, demonstrating how it can intelligently combine the strengths of **Traditional RAG** and **Agentic RAG** approaches. By routing queries based on their complexity, Adaptive RAG aims to optimize the balance between **latency** (speed) and **quality** (accuracy and depth of analysis) for different types of user questions.\n",
        "\n",
        "\n",
        "\n",
        "We use an **Admin trigger troubleshooting use case** to illustrate these concepts, leveraging:\n",
        "-   **ChromaDB**: A vector database for efficient storage and retrieval of troubleshooting knowledge.\n",
        "-   **OpenAI GPT-4o-mini**: A powerful language model for generating responses and executing diagnostic steps.\n",
        "\n",
        "The scenario is as follows:\n",
        "1. There is a trigger in the system that failed.\n",
        "2. There is a document that explains how to troubleshoot triggers.\n",
        "3. The trigger setting and logs are available to the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## Understanding the RAG Approaches\n",
        "\n",
        "-   **Traditional RAG**:\n",
        "    -   **Process**: Simple, single-step approach. Retrieves relevant documents from a vector database and uses an LLM to generate an answer based *only* on the retrieved context.\n",
        "    -   **Characteristics**: Generally **fast** and **low cost** due to minimal LLM interaction. Provides **generic answers** without analyzing specific system states.\n",
        "\n",
        "-   **Agentic RAG**:\n",
        "    -   **Process**: A multi-step reasoning process that involves multiple LLM calls. It retrieves relevant information from the troubleshooting guides *and* analyzes provided state data (like trigger settings and logs) to form a diagnosis. The LLM acts as an \"agent\" to plan and execute checks.\n",
        "    -   **Characteristics**: **Thorough** and provides **specific solution**. However, it is **slower** and **more costly** due to the increased number of steps and LLM interactions.\n",
        "\n",
        "-   **Adaptive RAG**:\n",
        "    -   **Process**: Introduces an intelligent router that assesses the **complexity** of the incoming user query. Simple queries are directed to the faster Traditional RAG path, while complex queries requiring deeper analysis are sent to the more thorough Agentic RAG path.\n",
        "    -   **Characteristics**: Aims to achieve the **best balance** between speed and quality by using the most appropriate method for each query type.\n",
        "\n",
        "---\n",
        "\n",
        "## Demonstration and Comparison\n",
        "\n",
        "The notebook demonstrates these approaches by:\n",
        "1.  Setting up a knowledge base in ChromaDB and simulating trigger state data.\n",
        "2.  Implementing and running both Traditional and Agentic RAG methods on a troubleshooting query.\n",
        "3.  Comparing their performance metrics (Latency, LLM Calls) and output quality.\n",
        "4.  Implementing a simple Hybrid RAG router and testing how it routes different types of queries.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "-   No single RAG method is ideal for all situations; there's an inherent **latency vs quality tradeoff**.\n",
        "-   Traditional RAG is effective for quick information retrieval (simple queries).\n",
        "-   Agentic RAG is powerful for complex problem-solving requiring state analysis.\n",
        "-   **Hybrid RAG** provides a practical solution to this tradeoff by dynamically choosing the optimal approach per query.\n",
        "-   Effective **query complexity assessment** is fundamental to a successful Hybrid RAG implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e53695e"
      },
      "source": [
        "## Comparison Metrics and Summary\n",
        "\n",
        "The demonstration highlights the differences between the RAG approaches based on the following metrics observed during the runs:\n",
        "\n",
        "-   **Quality**: The relevance, specificity, and depth of the generated answer.\n",
        "-   **Latency**: The time taken to generate a response.\n",
        "-   **LLM Calls**: The number of times the language model is invoked.\n",
        "-   **Cost**: An estimation of the cost based on the number of LLM calls and assumed token usage.\n",
        "-   **Complexity Handled**: The type of queries each method is best suited for.\n",
        "-   **State Analysis**: Whether the method incorporates analysis of the provided system state.\n",
        "-   **Steps**: The number and nature of the steps involved in generating a response.\n",
        "\n",
        "Here is a summary table comparing the approaches with metrics from the runs:\n",
        "\n",
        "| Characteristic     | Traditional RAG                 | Agentic RAG                     | Adaptive RAG                      |\n",
        "| :----------------- | :------------------------------ | :------------------------------ | :------------------------------ |\n",
        "| **Quality**        | Generic answers                 | Specific diagnosis              | Varies based on routing         |\n",
        "| **Latency**        | 3766 ms                        | 14316 ms                       | 8721 ms (average)              |\n",
        "| **LLM Calls**      | 1                               | 6                               | ~3.5 (average)                  |\n",
        "| **Estimated Cost** | 0.02 cents                         | 0.12 cents              | 0.07 cents                          |\n",
        "| **Complexity Handled** | Simple queries                  | Complex queries                 | Routes based on query complexity |\n",
        "| **State Analysis** | No                              | Yes                             | Yes (when routed to Agentic)    |\n",
        "| **Steps**          | Single retrieval and generation | Multi-step reasoning and checks | Routes to appropriate method    |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-HjpylNCW5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c68a12-1249-409b-99c8-3eec282a09ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Dependencies\n",
        "# ============================================================================\n",
        "\n",
        "!pip install chromadb openai python-dotenv -q\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGDcrQ4CW5X",
        "outputId": "63e20065-8e16-4bc6-8bb1-1947c3655cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Imports loaded\n",
            "✓ OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Setup and Imports\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "print(\"✓ Imports loaded\")\n",
        "print(\"✓ OpenAI client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHUS8AycCW5Y",
        "outputId": "d3daa928-77c0-4fc0-ddde-181242154f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Knowledge base created\n",
            "  - 4 troubleshooting documents\n",
            "  - Trigger state with 3 logs\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Create Knowledge Base\n",
        "# ============================================================================\n",
        "\n",
        "# Troubleshooting guide as a single string\n",
        "troubleshooting_guide_text = \"\"\"Trigger Disabled Issue: If trigger is not firing, first check if the trigger is enabled.\n",
        "Go to Settings > Automation > Triggers and look for the toggle switch. It should be ON/green.\n",
        "Disabled triggers never fire regardless of conditions.\n",
        "\n",
        "Condition Matching: Triggers only fire when conditions match ticket data.\n",
        "With ALL logic, every condition must be true. With ANY logic, at least one condition must be true.\n",
        "Compare your trigger conditions against the actual ticket field values carefully.\n",
        "\n",
        "Logic Types Explained: ALL logic means every condition must match (AND).\n",
        "For example, status=new AND priority=high means both must be true.\n",
        "ANY logic means at least one condition must match (OR).\n",
        "Common mistake: using ALL when you meant ANY.\n",
        "\n",
        "Execution Logs: Check trigger logs to see which tickets were evaluated,\n",
        "whether the trigger fired, and the specific reason it didn't fire.\n",
        "Logs are found in Settings > Automation > Logs and show execution history with error details.\n",
        "\"\"\"\n",
        "\n",
        "# Split the guide into individual documents based on paragraphs\n",
        "troubleshooting_docs_content = troubleshooting_guide_text.strip().split('\\n\\n')\n",
        "\n",
        "# Create a list of document dictionaries with IDs and metadata\n",
        "troubleshooting_docs = [\n",
        "    {\n",
        "        \"id\": f\"doc_{i+1}\",\n",
        "        \"content\": content,\n",
        "        \"metadata\": {\"category\": \"troubleshooting\"} # Using a generic category for now\n",
        "    }\n",
        "    for i, content in enumerate(troubleshooting_docs_content)\n",
        "]\n",
        "\n",
        "\n",
        "# Trigger state (actual settings and logs)\n",
        "trigger_state = {\n",
        "    \"trigger_settings\": {\n",
        "        \"id\": \"trigger_001\",\n",
        "        \"name\": \"Auto-assign high priority tickets\",\n",
        "        \"enabled\": True,\n",
        "        \"conditions\": [\n",
        "            {\"field\": \"status\", \"operator\": \"equals\", \"value\": \"new\"},\n",
        "            {\"field\": \"priority\", \"operator\": \"equals\", \"value\": \"high\"}\n",
        "        ],\n",
        "        \"logic\": \"ALL\",\n",
        "        \"actions\": [\"assign_to_team_a\"]\n",
        "    },\n",
        "    \"execution_logs\": [\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_123\",\n",
        "            \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: priority is 'medium', expected 'high'\"\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_124\",\n",
        "            \"timestamp\": \"2025-01-15T11:15:00Z\",\n",
        "            \"fired\": True,\n",
        "            \"actions_executed\": [\"Assigned to Team A\"]\n",
        "        },\n",
        "        {\n",
        "            \"ticket_id\": \"TKT_125\",\n",
        "            \"timestamp\": \"2025-01-15T14:22:00Z\",\n",
        "            \"fired\": False,\n",
        "            \"reason\": \"Condition mismatch: status is 'open', expected 'new'\"\n",
        "        }\n",
        "    ],\n",
        "    \"recent_tickets\": [\n",
        "        {\"id\": \"TKT_123\", \"status\": \"new\", \"priority\": \"medium\"},\n",
        "        {\"id\": \"TKT_124\", \"status\": \"new\", \"priority\": \"high\"},\n",
        "        {\"id\": \"TKT_125\", \"status\": \"open\", \"priority\": \"high\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"✓ Knowledge base created\")\n",
        "print(f\"  - {len(troubleshooting_docs)} troubleshooting documents\")\n",
        "print(f\"  - Trigger state with {len(trigger_state['execution_logs'])} logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crlwWDY3CW5Y",
        "outputId": "70962efa-bbb6-45b1-ecb3-9f12d759cd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ChromaDB initialized\n",
            "  - Collection: trigger_troubleshooting\n",
            "  - Documents: 4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Setup ChromaDB Vector Database\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Reset to start fresh\n",
        "chroma_client.reset()\n",
        "\n",
        "# Create collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"trigger_troubleshooting\",\n",
        "    metadata={\"description\": \"Admin trigger troubleshooting knowledge base\"}\n",
        ")\n",
        "\n",
        "# Add documents to collection\n",
        "collection.add(\n",
        "    documents=[doc[\"content\"] for doc in troubleshooting_docs],\n",
        "    ids=[doc[\"id\"] for doc in troubleshooting_docs],\n",
        "    metadatas=[doc[\"metadata\"] for doc in troubleshooting_docs]\n",
        ")\n",
        "\n",
        "print(\"✓ ChromaDB initialized\")\n",
        "print(f\"  - Collection: {collection.name}\")\n",
        "print(f\"  - Documents: {collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC15P7FqCW5Y",
        "outputId": "abc25db2-9b58-4805-8e38-8aa2be8fbed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Helper functions ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Helper Functions for LLM and Retrieval\n",
        "# ============================================================================\n",
        "\n",
        "def call_llm(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 500) -> str:\n",
        "    \"\"\"Call OpenAI API\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def retrieve_docs(query: str, n_results: int = 2) -> List[Dict]:\n",
        "    \"\"\"Retrieve relevant documents from ChromaDB\"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    docs = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        docs.append({\n",
        "            'id': results['ids'][0][i],\n",
        "            'content': results['documents'][0][i],\n",
        "            'metadata': results['metadatas'][0][i],\n",
        "            'distance': results['distances'][0][i] if 'distances' in results else None\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "print(\"✓ Helper functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-rI7RoCW5Y",
        "outputId": "b8237d89-3c01-4a81-fa14-8684905c8bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Traditional RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Traditional RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class TraditionalRAG:\n",
        "    \"\"\"\n",
        "    Traditional RAG: Single retrieval + single generation\n",
        "    - Retrieve relevant docs from vector DB\n",
        "    - Generate answer in one LLM call\n",
        "    - Fast but generic (no state analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "\n",
        "        # Step 1: Retrieve relevant docs\n",
        "        docs = retrieve_docs(query, n_results=2)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "\n",
        "        # Step 2: Generate answer\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "        prompt = f\"\"\"You are a helpful admin assistant. Answer the user's question based on this troubleshooting guide.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Troubleshooting Guide:\n",
        "{context}\n",
        "\n",
        "Provide a helpful answer:\"\"\"\n",
        "\n",
        "        answer = call_llm(prompt)\n",
        "        steps.append(\"Generated answer with LLM\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Traditional RAG\",\n",
        "            \"retrieval_calls\": 1,\n",
        "            \"llm_calls\": 1,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "trad_rag = TraditionalRAG()\n",
        "print(\"✓ Traditional RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpGeXub4CW5Y",
        "outputId": "67d039db-7631-429a-f216-93ecaaf5b98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Agentic RAG ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Agentic RAG Implementation\n",
        "# ============================================================================\n",
        "\n",
        "class AgenticRAG:\n",
        "    \"\"\"\n",
        "    Agentic RAG: Multi-step reasoning with state analysis\n",
        "    - Retrieve troubleshooting guide\n",
        "    - Create diagnostic plan (LLM call #1)\n",
        "    - Execute state checks (LLM calls #2-5)\n",
        "    - Synthesize diagnosis (LLM call #6)\n",
        "    - Slower but provides specific diagnosis\n",
        "    \"\"\"\n",
        "\n",
        "    def answer(self, query: str, state: Dict = None) -> Dict:\n",
        "        start_time = time.time()\n",
        "        steps = []\n",
        "        llm_calls = 0\n",
        "        retrieval_calls = 0\n",
        "\n",
        "        if state is None:\n",
        "            state = trigger_state\n",
        "\n",
        "        # Step 1: Retrieve troubleshooting docs\n",
        "        docs = retrieve_docs(query, n_results=3)\n",
        "        steps.append(f\"Retrieved {len(docs)} documents from ChromaDB\")\n",
        "        retrieval_calls += 1\n",
        "\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in docs])\n",
        "\n",
        "        # Step 2: Create diagnostic plan (LLM call #1)\n",
        "        plan_prompt = f\"\"\"Based on the user's question and troubleshooting guide, what specific checks should we run?\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Guide:\n",
        "{context}\n",
        "\n",
        "Return ONLY a JSON array of 4 specific checks to run against the trigger state.\n",
        "Example format: [\"check one\", \"check two\", \"check three\", \"check four\"]\n",
        "DO NOT include any other text or formatting outside the JSON array.\"\"\"\n",
        "\n",
        "        plan_response = call_llm(plan_prompt, max_tokens=200)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Created diagnostic plan\")\n",
        "\n",
        "        try:\n",
        "            diagnostic_plan = json.loads(plan_response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Diagnostic plan could not be parsed. Response was: {plan_response}\")\n",
        "            print(f\"Parsing error: {e}\")\n",
        "            # Fallback plan if parsing fails\n",
        "            diagnostic_plan = [\"Check enabled status\", \"Analyze execution logs\", \"Compare conditions with tickets\", \"Verify logic type\"]\n",
        "            steps.append(\"Using fallback diagnostic plan due to parsing error\")\n",
        "\n",
        "\n",
        "        # Step 3: Execute each diagnostic check\n",
        "        findings = []\n",
        "\n",
        "        # Check 1: Is enabled?\n",
        "        check_prompt = f\"\"\"Check if trigger is enabled.\n",
        "\n",
        "Trigger settings: {json.dumps(state['trigger_settings'], indent=2)}\n",
        "\n",
        "Answer in one sentence: Is the trigger enabled?\"\"\"\n",
        "        finding = call_llm(check_prompt, max_tokens=50)\n",
        "        findings.append(f\"Enabled status: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #1: Verified enabled status\")\n",
        "\n",
        "        # Check 2: Analyze logs\n",
        "        logs_prompt = f\"\"\"Analyze these execution logs.\n",
        "\n",
        "Logs: {json.dumps(state['execution_logs'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: What do the logs show about trigger firing?\"\"\"\n",
        "        finding = call_llm(logs_prompt, max_tokens=100)\n",
        "        findings.append(f\"Log analysis: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #2: Analyzed execution logs\")\n",
        "\n",
        "        # Check 3: Compare conditions\n",
        "        conditions_prompt = f\"\"\"Compare trigger conditions against actual tickets.\n",
        "\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "Recent tickets: {json.dumps(state['recent_tickets'], indent=2)}\n",
        "\n",
        "Answer in 2-3 sentences: Which tickets match the conditions?\"\"\"\n",
        "        finding = call_llm(conditions_prompt, max_tokens=150)\n",
        "        findings.append(f\"Condition matching: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #3: Compared conditions vs tickets\")\n",
        "\n",
        "        # Check 4: Verify logic type\n",
        "        logic_prompt = f\"\"\"Explain the logic type.\n",
        "\n",
        "Logic type: {state['trigger_settings']['logic']}\n",
        "Conditions: {json.dumps(state['trigger_settings']['conditions'], indent=2)}\n",
        "\n",
        "Answer in 1-2 sentences: What does this logic type mean?\"\"\"\n",
        "        finding = call_llm(logic_prompt, max_tokens=100)\n",
        "        findings.append(f\"Logic type: {finding}\")\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Check #4: Verified logic type\")\n",
        "\n",
        "\n",
        "        # Step 4: Synthesize final diagnosis (LLM call #6)\n",
        "        synthesis_prompt = f\"\"\"Based on all the findings, provide a specific diagnosis.\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Findings:\n",
        "{chr(10).join(f'{i+1}. {f}' for i, f in enumerate(findings))}\n",
        "\n",
        "Provide a specific, actionable answer that:\n",
        "1. Explains if the trigger is working correctly or not\n",
        "2. Gives evidence from the actual state\n",
        "3. Explains why certain tickets didn't fire\n",
        "4. Suggests concrete next steps if needed\"\"\"\n",
        "\n",
        "        answer = call_llm(synthesis_prompt, max_tokens=600)\n",
        "        llm_calls += 1\n",
        "        steps.append(\"Synthesized final diagnosis\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"method\": \"Agentic RAG\",\n",
        "            \"retrieval_calls\": retrieval_calls,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"docs_retrieved\": len(docs),\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"steps\": steps,\n",
        "            \"findings\": findings,\n",
        "            \"retrieved_docs\": docs\n",
        "        }\n",
        "\n",
        "agentic_rag = AgenticRAG()\n",
        "print(\"✓ Agentic RAG ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb1nEW_ZCW5Y",
        "outputId": "b72dd6df-0bbd-4d2a-8c8d-da5cb4c9390c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Hybrid RAG router ready\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Hybrid RAG Router\n",
        "# ============================================================================\n",
        "\n",
        "class HybridRAG:\n",
        "    \"\"\"\n",
        "    Hybrid RAG: Intelligent routing\n",
        "    - Simple queries → Traditional RAG\n",
        "    - Complex queries → Agentic RAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, complexity_threshold: float = 0.5):\n",
        "        self.traditional = TraditionalRAG()\n",
        "        self.agentic = AgenticRAG()\n",
        "        self.threshold = complexity_threshold\n",
        "\n",
        "    def assess_complexity(self, query: str) -> float:\n",
        "        \"\"\"Assess query complexity (0.0 = simple, 1.0 = complex)\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Simple queries\n",
        "        if any(word in query_lower for word in [\"where\", \"what is\", \"how to find\"]):\n",
        "            return 0.3\n",
        "\n",
        "        # Complex queries\n",
        "        if any(word in query_lower for word in [\"why\", \"not working\", \"not firing\", \"issue\"]):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.5\n",
        "\n",
        "    def answer(self, query: str, verbose: bool = True) -> Dict:\n",
        "        complexity = self.assess_complexity(query)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Complexity: {complexity:.2f} (threshold: {self.threshold})\")\n",
        "\n",
        "        if complexity < self.threshold:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Traditional RAG (simple query)\\n\")\n",
        "            result = self.traditional.answer(query)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"→ Routing to: Agentic RAG (complex query)\\n\")\n",
        "            result = self.agentic.answer(query)\n",
        "\n",
        "        result['complexity'] = complexity\n",
        "        return result\n",
        "\n",
        "hybrid = HybridRAG(complexity_threshold=0.5)\n",
        "print(\"✓ Hybrid RAG router ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmB8_DwCW5Y",
        "outputId": "38683b87-e71f-46fc-bde9-aa08953cdbea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPARISON: Traditional RAG vs Agentic RAG\n",
            "======================================================================\n",
            "Query: Why isn't my trigger firing?\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "METHOD 1: Traditional RAG\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 2 documents from ChromaDB\n",
            "  2. Generated answer with LLM\n",
            "\n",
            "Answer:\n",
            "If your trigger isn't firing, there are a couple of key areas to check:\n",
            "\n",
            "1. **Trigger Status**: First, ensure that your trigger is enabled. You can do this by navigating to **Settings > Automation > Triggers** and looking for the toggle switch next to your trigger. It should be ON (green). If it's OFF, simply switch it to ON.\n",
            "\n",
            "2. **Condition Matching**: Next, review the conditions set for your trigger. Triggers only activate when the conditions match the ticket data. If you're using ALL logic, every condition must be true for the trigger to fire. If you're using ANY logic, at least one condition must be true. Double-check that the conditions you have set align with the actual values in the ticket fields.\n",
            "\n",
            "By verifying these two aspects, you should be able to identify why your trigger isn't firing. If you need further assistance, feel free to ask!\n",
            "\n",
            "📊 Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 1\n",
            "   Latency: 3862.7ms\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "METHOD 2: Agentic RAG\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Steps:\n",
            "  1. Retrieved 3 documents from ChromaDB\n",
            "  2. Created diagnostic plan\n",
            "  3. Check #1: Verified enabled status\n",
            "  4. Check #2: Analyzed execution logs\n",
            "  5. Check #3: Compared conditions vs tickets\n",
            "  6. Check #4: Verified logic type\n",
            "  7. Synthesized final diagnosis\n",
            "\n",
            "Answer:\n",
            "### Diagnosis of Trigger Firing Issue\n",
            "\n",
            "1. **Is the Trigger Working Correctly?**\n",
            "   Yes, the trigger is functioning as intended. It is enabled and successfully fired for ticket TKT_124, which met all specified conditions.\n",
            "\n",
            "2. **Evidence from the Actual State:**\n",
            "   - The logs show that the trigger fired successfully for TKT_124, which has a status of \"new\" and a priority of \"high.\" \n",
            "   - The other two tickets, TKT_123 and TKT_125, did not meet the criteria set by the trigger, which requires both conditions (status = \"new\" and priority = \"high\") to be satisfied.\n",
            "\n",
            "3. **Why Certain Tickets Didn't Fire:**\n",
            "   - **TKT_123:** This ticket did not fire because it did not meet the priority condition. The priority for TKT_123 is not \"high,\" which is required for the trigger to activate.\n",
            "   - **TKT_125:** This ticket failed to fire due to a mismatch in the status condition. TKT_125 does not have a status of \"new,\" which is necessary for the trigger to be satisfied.\n",
            "\n",
            "4. **Concrete Next Steps:**\n",
            "   - **Review Ticket Conditions:** Examine the conditions for TKT_123 and TKT_125 to determine if they can be adjusted to meet the trigger's criteria. If these tickets should also be processed by the trigger, consider changing their priority or status accordingly.\n",
            "   - **Adjust Trigger Logic (if necessary):** If the current logic of requiring both conditions is too restrictive, consider modifying the trigger to allow for more flexibility (e.g., using \"ANY\" logic instead of \"ALL\" if appropriate).\n",
            "   - **Monitor Future Ticket Evaluations:** Continue to monitor the logs for future ticket evaluations to ensure that the trigger is firing as expected and to identify any additional tickets that may not meet the criteria. \n",
            "\n",
            "By following these steps, you can ensure that the trigger functions effectively for all relevant tickets.\n",
            "\n",
            "📊 Performance:\n",
            "   Retrieval calls: 1\n",
            "   LLM calls: 6\n",
            "   Latency: 18304.2ms\n",
            "\n",
            "======================================================================\n",
            "TRADEOFF ANALYSIS\n",
            "======================================================================\n",
            "⚡ Speed: Traditional is 4.7x faster\n",
            "🤖 LLM calls: Agentic uses 6x more\n",
            "🎯 Quality: Agentic provides specific diagnosis with state analysis\n",
            "💰 Cost: Agentic uses 6x more tokens\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Run Comparison Demo\n",
        "# ============================================================================\n",
        "\n",
        "def compare_methods(query: str):\n",
        "    \"\"\"Compare Traditional vs Agentic RAG\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON: Traditional RAG vs Agentic RAG\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Query: {query}\\n\")\n",
        "\n",
        "    # Traditional\n",
        "    print(\"─\"*70)\n",
        "    print(\"METHOD 1: Traditional RAG\")\n",
        "    print(\"─\"*70)\n",
        "    trad_result = trad_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(trad_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{trad_result['answer']}\")\n",
        "    print(f\"\\n📊 Performance:\")\n",
        "    print(f\"   Retrieval calls: {trad_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {trad_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {trad_result['latency_ms']}ms\")\n",
        "\n",
        "    # Agentic\n",
        "    print(\"\\n\" + \"─\"*70)\n",
        "    print(\"METHOD 2: Agentic RAG\")\n",
        "    print(\"─\"*70)\n",
        "    agentic_result = agentic_rag.answer(query)\n",
        "\n",
        "    print(f\"\\nSteps:\")\n",
        "    for i, step in enumerate(agentic_result['steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nAnswer:\\n{agentic_result['answer']}\")\n",
        "    print(f\"\\n📊 Performance:\")\n",
        "    print(f\"   Retrieval calls: {agentic_result['retrieval_calls']}\")\n",
        "    print(f\"   LLM calls: {agentic_result['llm_calls']}\")\n",
        "    print(f\"   Latency: {agentic_result['latency_ms']}ms\")\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRADEOFF ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    speedup = agentic_result['latency_ms'] / trad_result['latency_ms']\n",
        "    print(f\"⚡ Speed: Traditional is {speedup:.1f}x faster\")\n",
        "    print(f\"🤖 LLM calls: Agentic uses {agentic_result['llm_calls']}x more\")\n",
        "    print(f\"🎯 Quality: Agentic provides specific diagnosis with state analysis\")\n",
        "    print(f\"💰 Cost: Agentic uses {agentic_result['llm_calls']}x more tokens\")\n",
        "\n",
        "# Run comparison\n",
        "compare_methods(\"Why isn't my trigger firing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcNxQ-s1CW5Z",
        "outputId": "13b74418-a6a8-4cac-8083-dd418e422849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "HYBRID RAG ROUTER TEST\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Query: Where can I find triggers?\n",
            "======================================================================\n",
            "Complexity: 0.30 (threshold: 0.5)\n",
            "→ Routing to: Traditional RAG (simple query)\n",
            "\n",
            "\n",
            "Method chosen: Traditional RAG\n",
            "Final Answer:\n",
            "You can find triggers by navigating to Settings > Automation > Triggers. Here, you can view all your triggers and check if they are enabled. Make sure the toggle switch for each trigger is ON (green) to ensure they fire correctly. If you need to check the execution history or any issues with triggers not firing, you can look at the execution logs by going to Settings > Automation > Logs. This will show you which tickets were evaluated and any error details if a trigger didn't fire.\n",
            "Performance: 1 LLM calls, 2162.87ms\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: My trigger isn't firing, what's wrong?\n",
            "======================================================================\n",
            "Complexity: 0.50 (threshold: 0.5)\n",
            "→ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Issue\n",
            "\n",
            "1. **Is the Trigger Working Correctly?**\n",
            "   - Yes, the trigger is functioning correctly based on the defined conditions. It successfully fired for ticket TKT_124, which met the specified criteria.\n",
            "\n",
            "2. **Evidence from the Actual State:**\n",
            "   - The logs show that the trigger is enabled and has processed three tickets. Only TKT_124 met the conditions of having a status of \"new\" and a priority of \"high,\" which resulted in it firing and assigning the ticket to Team A. The other two tickets did not meet the criteria, which is why the trigger did not fire for them.\n",
            "\n",
            "3. **Why Certain Tickets Didn't Fire:**\n",
            "   - **TKT_123:** This ticket has a medium priority, which does not satisfy the condition of having a \"high\" priority. Therefore, it did not match all the required conditions.\n",
            "   - **TKT_125:** This ticket has an open status, which does not match the required condition of having a status of \"new.\" As a result, it also did not meet the criteria for the trigger to fire.\n",
            "\n",
            "4. **Concrete Next Steps:**\n",
            "   - **Review and Adjust Conditions:** If you want TKT_123 and TKT_125 to trigger actions, consider revising the conditions of the trigger. For example, you could create additional triggers that accommodate different priority levels or statuses.\n",
            "   - **Testing:** After making any adjustments, conduct tests with various ticket scenarios to ensure that the trigger works as intended across different conditions.\n",
            "   - **Documentation:** Document the current conditions and any changes made for future reference and troubleshooting.\n",
            "   - **Monitoring:** Continue to monitor the logs after implementing changes to ensure that the trigger behaves as expected for all relevant tickets. \n",
            "\n",
            "By following these steps, you can enhance the functionality of your trigger and ensure that it fires for the appropriate tickets.\n",
            "Performance: 6 LLM calls, 20274.25ms\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: Why did trigger fire for some tickets but not others?\n",
            "======================================================================\n",
            "Complexity: 0.80 (threshold: 0.5)\n",
            "→ Routing to: Agentic RAG (complex query)\n",
            "\n",
            "\n",
            "Method chosen: Agentic RAG\n",
            "Final Answer:\n",
            "### Diagnosis of Trigger Functionality\n",
            "\n",
            "1. **Trigger Functionality**: The trigger is functioning correctly as designed. It is enabled and is operating according to the specified conditions.\n",
            "\n",
            "2. **Evidence from Actual State**: The log analysis shows that the trigger successfully fired for ticket TKT_124, which met both required conditions: it had a status of \"new\" and a priority of \"high.\" This indicates that the trigger is correctly identifying and acting on tickets that fulfill the specified criteria.\n",
            "\n",
            "3. **Reasons for Non-Firing Tickets**:\n",
            "   - **TKT_123**: This ticket did not fire because it did not meet the priority condition. The priority for TKT_123 was not \"high,\" which is necessary for the trigger to activate.\n",
            "   - **TKT_125**: This ticket failed to trigger due to its status. TKT_125 did not have the status of \"new,\" which is another critical requirement for the trigger to fire.\n",
            "\n",
            "4. **Next Steps**:\n",
            "   - **Review Ticket Conditions**: Evaluate the conditions of TKT_123 and TKT_125 to determine if adjustments can be made to their status or priority to meet the trigger's criteria in future processing.\n",
            "   - **Consider Additional Triggers**: If there are other conditions under which tickets should be processed, consider creating additional triggers that account for different combinations of status and priority.\n",
            "   - **Monitor Future Ticket Processing**: Continue to monitor ticket processing to ensure that the trigger behaves as expected and to identify any additional tickets that may not meet the criteria.\n",
            "   - **Documentation and Training**: Ensure that all relevant team members are aware of the trigger conditions and the importance of ticket status and priority, potentially providing training or documentation to prevent future mismatches.\n",
            "\n",
            "By following these steps, you can enhance the effectiveness of the trigger system and ensure that it operates smoothly for all relevant tickets.\n",
            "Performance: 6 LLM calls, 17854.34ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Test Hybrid Router\n",
        "# ============================================================================\n",
        "\n",
        "test_queries = [\n",
        "    \"Where can I find triggers?\",\n",
        "    \"My trigger isn't firing, what's wrong?\",\n",
        "    \"Why did trigger fire for some tickets but not others?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYBRID RAG ROUTER TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for query in test_queries:\n",
        "    result = hybrid.answer(query, verbose=True)\n",
        "    print(f\"\\nMethod chosen: {result['method']}\")\n",
        "    print(f\"Final Answer:\\n{result['answer']}\") # Added to display the final answer\n",
        "    print(f\"Performance: {result['llm_calls']} LLM calls, {result['latency_ms']}ms\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcSuDfk_yDx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c2e598"
      },
      "source": [
        "## Hybrid RAG Router Test Performance\n",
        "\n",
        "Based on the last run of the Hybrid RAG router test (Cell 10), here is the performance for each query:\n",
        "\n",
        "| Query                                        | Complexity | Method Chosen     | LLM Calls | Latency (ms) |\n",
        "| :------------------------------------------- | :--------- | :---------------- | :-------- | :----------- |\n",
        "| Where can I find triggers?                   | 0.30       | Traditional RAG   | 1         | 2162.87      |\n",
        "| My trigger isn't firing, what's wrong?       | 0.50       | Agentic RAG       | 6         | 20274.25     |\n",
        "| Why did trigger fire for some tickets but not others? | 0.80       | Agentic RAG       | 6         | 17854.34     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7389fa6b"
      },
      "source": [
        "## Average Performance Comparison of RAG Approaches\n",
        "\n",
        "Based on the comparison run (Cell 9) and the hybrid test run (Cell 10), here is an estimated average performance comparison across the three RAG approaches demonstrated in this notebook.\n",
        "\n",
        "*Note: Averages for Hybrid RAG are based on the sample queries tested and depend heavily on the complexity routing.*\n",
        "\n",
        "| RAG Approach      | Average LLM Calls | Average Latency (ms) |\n",
        "| :---------------- | :---------------- | :------------------- |\n",
        "| Traditional RAG   | 1                 | ~3863                |\n",
        "| Agentic RAG       | 6                 | ~18080 (Average of two runs in Cell 10) |\n",
        "| Adaptive (Hybrid) | ~4.3 (Average of LLM calls in Cell 10) | ~13430 (Average of latencies in Cell 10) |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}